{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: Georgia; font-size:3em;color:#2462C0; font-style:bold\">\n",
    "Character-Level Language Model</h1><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character-level language model's task is to predict next character given all previous characters in a sequence of data.\n",
    "RNN model family are very powerful and expressive in which they remember and process past information through their high dimensional hidden state units.\n",
    "Generates names character by character.\n",
    "Statistical language models try to capture the statistical structure of training text it's trained on. They play a crucial part in NLP tasks such as machine translation and speech recognition. The main goal of any language model is to learn the joint probability distribution of sequences of characters/words in a training text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, given a training sequence (x1, . . . , xT ), the RNN uses\n",
    "the sequence of its output vectors (o1, . . . , oT ) to obtain\n",
    "a sequence of predictive distributions P(xt+1|xâ‰¤t) =\n",
    "softmax(ot), where the softmax distribution is defined by\n",
    "P(softmax(ot) = j) = exp(o(j)t)/Pkexp(o(k)t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate how the character-level language model works using my first name (\"imad\") as an example. We first build a vocabulary dictionary using all the unique letters (since we're assuming all names consist of english letters only) of the names in the corpus as keys and the index of each letter starting from zero (since python is a zero index language) in ascending order. For our example, the vocabulary dictionary would be: {\"a\": 0, \"d\": 1, \"i\": 2, \"m\": 3}. Therefore, \"imad\" would become a list of the following integers: [2, 3, 0, 1].\n",
    "\n",
    "Second, first letter is always zero vector, i.e. $x^1 = \\vec{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    "<img src=\"posts_images/char_level_model/char_level_example.PNG\"; style=\"width: 800px; height: 600px\"><br>\n",
    "<caption><center><u><font color=\"purple\">**Figure 1:**</font></u> Illustrative example of character-level language model using RNN</center></caption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\")\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def clip_gradients(gradients, max_value):\n",
    "    \"\"\"\n",
    "    Implements gradient clipping element-wise on gradients values\n",
    "    to be between the interval [-max_value, max_value].\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    gradients : python dict\n",
    "        dictionary that stores all the gradients.\n",
    "    max_value : scalar\n",
    "        edge of the interval [-max_value, max_value].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradients : python dict\n",
    "        dictionary where all gradients were clipped.\n",
    "    \"\"\"\n",
    "    for grad in gradients.keys():\n",
    "        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n"
     ]
    }
   ],
   "source": [
    "# Test clip_gradients function\n",
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip_gradients(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Implements softmax on the array z and returns normalized probability.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    z : array-like\n",
    "        array contains logits.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    probs : array\n",
    "        array containg the probability of each element from the logits array.\n",
    "    \n",
    "    \"\"\"\n",
    "    e_z = np.exp(z)\n",
    "    probs = e_z / np.sum(e_z)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sample(parameters, idx_to_chars, chars_to_idx, n):\n",
    "    \"\"\"\n",
    "    Implements sampling of a squence of characters until reach\n",
    "    EOS (which is new line) or n characters length. The sampling\n",
    "    will be based on the probability distribution output of RNN.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    parameters : python dict\n",
    "        dictionary storing all the parameters of the model.\n",
    "    idx_to_chars : python dict\n",
    "        dictionary mapping indices to characters.\n",
    "    chars_to_idx : python dict\n",
    "        dictionary mapping characters to indices.\n",
    "    n : scalar\n",
    "        max number of characters to output if RNN didn't output EOS.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sequence : str\n",
    "        sequence of characters sampled.\n",
    "    \"\"\"\n",
    "    # Retrienve parameters, shapes, and vocab size\n",
    "    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n",
    "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
    "    n_h, n_x = Wxh.shape\n",
    "    vocab_size = c.shape[0]\n",
    "\n",
    "    # Initialize a0 and x1 to zero vectors\n",
    "    h_prev = np.zeros((n_h, 1))\n",
    "    x = np.zeros((n_x, 1))\n",
    "\n",
    "    # Initialize empty sequence\n",
    "    sequence = \"\"\n",
    "\n",
    "    # Initialize counter to keep track of number of sequences generated\n",
    "    # to not exceed n\n",
    "    counter = 0\n",
    "\n",
    "    while counter <= n:\n",
    "        # Fwd propagation\n",
    "        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n",
    "        o = np.dot(Why, h) + c\n",
    "        probs = softmax(o)\n",
    "\n",
    "        # Sample the index of the character using generated probs distribution\n",
    "        idx = np.random.choice(vocab_size, p=probs.ravel())\n",
    "\n",
    "        # Get the character of the sampled index\n",
    "        char = idx_to_chars[idx]\n",
    "\n",
    "        # Add the char to the sequence\n",
    "        sequence += char\n",
    "\n",
    "        # Check if we reached EOS\n",
    "        if char == \"\\n\":\n",
    "            break\n",
    "\n",
    "        # Update a_prev and x\n",
    "        h_prev = h\n",
    "        x = np.zeros((n_x, 1))\n",
    "        x[idx] = 1\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n",
      "(26, 1)\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'vivaegxzaek'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test sample function\n",
    "_, n_h = 20, 100\n",
    "Wxh, Whh, Why = np.random.randn(n_h, vocab_size), np.random.randn(n_h, n_h), np.random.randn(vocab_size, n_h)\n",
    "b, c = np.random.randn(n_h, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wxh\": Wxh, \"Whh\": Whh, \"Why\": Why, \"b\": b, \"c\": c}\n",
    "\n",
    "sample(parameters, idx_to_chars, chars_to_idx, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(vocab_size, hidden_layer_size):\n",
    "    \"\"\"\n",
    "    Initialze model's parameters. biases will be initialzed to zeros\n",
    "    and weights will be initialized to small random numbers from normal\n",
    "    distribution.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    vocab_size : int\n",
    "        size of the vocab dictionary.\n",
    "    hidden_layer_size : int\n",
    "        size of hidden units.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters : python dict\n",
    "        dictionary containing all the initialized parameters.\n",
    "            Whh -- hidden to hidden.\n",
    "            Wxh -- input to hidden.\n",
    "            b -- hidden bias.\n",
    "            Why -- hidden to output.\n",
    "            c -- output bias.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    parameters[\"Whh\"] = np.random.randn(\n",
    "        hidden_layer_size, hidden_layer_size) * 0.01\n",
    "    parameters[\"Wxh\"] = np.random.randn(hidden_layer_size, vocab_size) * 0.01\n",
    "    parameters[\"b\"] = np.zeros((hidden_layer_size, 1))\n",
    "    parameters[\"Why\"] = np.random.randn(vocab_size, hidden_layer_size) * 0.01\n",
    "    parameters[\"c\"] = np.zeros((vocab_size, 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def smooth_loss(loss, current_loss):\n",
    "    \"\"\"Compute the weighted average of the loss to smooth it out\n",
    "       since it will be noisy given that we'll use stochastic gradient\n",
    "       descent (training one name at a time). Average the loss over past\n",
    "       1000 names by using factor = 0.999\n",
    "    \"\"\"\n",
    "    factor = 0.999\n",
    "    \n",
    "    return factor * loss + (1 - factor) * current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     32
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"Whh\", \"Wxh\", \"b\", \"Why\", \"c\".\n",
    "                - values: numpy arrays of zeros of the same shape as the\n",
    "                          corresponding gradients/parameters.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    parameters : python dict\n",
    "        dictionary containing the parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    v : python dict\n",
    "        dictionary that will contain the exponentially weighted\n",
    "        average of the gradient.\n",
    "    s : python dict\n",
    "        dictionary that will contain the exponentially weighted\n",
    "        average of the squared gradient.\n",
    "    \"\"\"\n",
    "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for param_name in parameters_names:\n",
    "        v[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
    "        s[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
    "\n",
    "    return v, s\n",
    "\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate,\n",
    "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    parameters : python dict\n",
    "        dictionary containing parameters.\n",
    "    grads : python dict\n",
    "        dictionary containing gradients for each parameters.\n",
    "    v : python dict\n",
    "        Adam variable, moving average of the first gradient.\n",
    "    s : python dict\n",
    "        Adam variable, moving average of the squared gradient.\n",
    "    learning_rate : float\n",
    "        learning rate.\n",
    "    beta1 : float\n",
    "        exponential decay hyperparameter for the first moment estimates.\n",
    "    beta2 : float\n",
    "        exponential decay hyperparameter for the second moment estimates.\n",
    "    epsilon : float\n",
    "        hyperparameter preventing division by zero in Adam updates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters : python dict\n",
    "        dictionary containing updated parameters.\n",
    "    v : python dict\n",
    "        Adam variable, moving average of the first gradient.\n",
    "    s : python dict\n",
    "        Adam variable, moving average of the squared gradient.\n",
    "    \"\"\"\n",
    "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "\n",
    "    for param_name in parameters_names:\n",
    "        # update the moving average of both first gradient and squared gradient\n",
    "        v[\"d\" + param_name] = beta1 * v[\"d\" + param_name] +\\\n",
    "            (1 - beta1) * grads[\"d\" + param_name]\n",
    "        s[\"d\" + param_name] = beta2 * s[\"d\" + param_name] +\\\n",
    "            (1 - beta2) * np.square(grads[\"d\" + param_name])\n",
    "\n",
    "        # compute the corrected-bias estimate of the moving averages\n",
    "        v_corrected[\"d\" + param_name] = v[\"d\" + param_name] / (1 - beta1**t)\n",
    "        s_corrected[\"d\" + param_name] = s[\"d\" + param_name] / (1 - beta2**t)\n",
    "\n",
    "        # update parameters\n",
    "        parameters[param_name] -= (learning_rate * v_corrected[\"d\" + param_name])\\\n",
    "            / (np.sqrt(s_corrected[\"d\" + param_name] + epsilon))\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, y, h_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement one Forward pass on one name.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : list\n",
    "        list of integers for the index of the characters in the example\n",
    "        shifted one character to the right.\n",
    "    y : list\n",
    "        list of integers for the index of the characters in the example.\n",
    "    h_prev : array\n",
    "        last hidden state from the previous example.\n",
    "    \n",
    "    parameters : python dict\n",
    "        dictionary containing the parameters.\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        cross-entropy loss.\n",
    "    cache : tuple\n",
    "        contains three python dictionaries:\n",
    "            xs -- input of all time steps.\n",
    "            hs -- hidden state of all time steps.\n",
    "            probs -- probability distribution of each character at each time step.\n",
    "    \"\"\"\n",
    "    # Retrieve parameters\n",
    "    Wxh, Whh, bh = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"bh\"]\n",
    "    Why, by = parameters[\"Why\"], parameters[\"by\"]\n",
    "\n",
    "    # Initialize inputs, hidden state, output, and probabilities dictionaries\n",
    "    xs, hs, os, probs = {}, {}, {}, {}\n",
    "\n",
    "    # Initialize x0 to zero vector\n",
    "    xs[0] = np.zeros((vocab_size, 1))\n",
    "\n",
    "    # Initialize loss and assigns h_prev to last hidden state in hs\n",
    "    loss = 0\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "\n",
    "    # Forward pass: loop over all characters of the name\n",
    "    for t in range(len(x)):\n",
    "        # Convert to one-hot vector\n",
    "        if t > 0:\n",
    "            xs[t] = np.zeros((vocab_size, 1))\n",
    "            xs[t][x[t]] = 1\n",
    "        # Hidden state\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh)\n",
    "        # Logits\n",
    "        os[t] = np.dot(Why, hs[t]) + by\n",
    "        # Probs\n",
    "        probs[t] = softmax(ys[t])\n",
    "        # Loss\n",
    "        loss -= np.log(probs[t][y[t], 0])\n",
    "\n",
    "    cache = (xs, hs, probs)\n",
    "\n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def rnn_backward(y, parameters, cache):\n",
    "    \"\"\"\n",
    "    Implements Backpropagation on one name.\n",
    "    \"\"\"\n",
    "    # Retrieve xs, hs, and probs\n",
    "    xs, hs, probs = cache\n",
    "\n",
    "    # Initialize all gradients to zero\n",
    "    dh_next = np.zeros_like(hs[0])\n",
    "    \n",
    "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
    "    grads = {}\n",
    "    for param_name in parameters_names:\n",
    "        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
    "\n",
    "    # Iterate over all time steps in reverse order starting from Tx\n",
    "    for t in reversed(range(len(xs))):\n",
    "        dy = np.copy(probs[t])\n",
    "        dy[y[t]] -= 1\n",
    "        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n",
    "        grads[\"dc\"] += dy\n",
    "        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n",
    "        dhraw = (1 - hs[t] ** 2) * dh\n",
    "        grads[\"dWhh\"] += np.dot(dhraw, hs[t].T)\n",
    "        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n",
    "        grads[\"db\"] += dhraw\n",
    "        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n",
    "        # Clip the gradients using [-5, 5] as the interval\n",
    "        grads = clip_gradients(grads, 5)\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    for param in parameters.keys():\n",
    "        parameters[param] -= learning_rate * grads[\"d\" + param]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Training</h2><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dataset](http://deron.meranda.us/data/census-derived-all-first.txt) we'll be using has 5,163 names: 4,275 male names, 1,219 female names, and 331 names can be both female and male names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: Georgia; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Forward Propagation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using Stochastic Gradient Descent (SGD) where each batch consists only of one example. In other words, the RNN model will learn from each example separately, i.e. run both forward and backward pass on each example and update parameters accordingly.\n",
    "- Create vocabulary dictionary using the unique lower case letters.\n",
    "    - Create character to index dictionary that maps each characters to indices in an ascending order. For example, \"a\" would have index 0 (since python is a zero index language) and \"z\" would have index 25. We will use this dictionary in converting names into list of integers where each letter will be represented as one-hot vector.\n",
    "    - Create index to character dictionary that maps indices to characters. This dictionary will be used to convert the output of the RNN model into characters which will be translated into names.\n",
    "- Initialize parameters: weights will be initialized to small random numbers from standard normal distribution to break symmetry and make sure different hidden units learn different things. On the other hand, biases will be initialized to zeros.\n",
    "    - $W_{hh}$: weight matrix connecting previous hidden state $h^{t - 1}$ to current hidden state $h^t$.\n",
    "    - $W_{xh}$: weight matrix connecting input $x^t$ to hidden state $h^t$.\n",
    "    - $b$: hidden state bias vector.\n",
    "    - $W_{hy}$: weight matrix connecting hidden state $h^t$ to output $o^t$.\n",
    "    - $c$: output bias vector.\n",
    "- Convert input $x^t$ and the output $y^t$into one-hot vector each. The dimension of the one-hot vector is vocab_size x 1. Everything will be zero except for the index of the letter at (t) would be 1. In our case, $x^t$ would be the same as $y^t$ shifted to the right where $x^1 = \\vec{}0$; however, starting from $t = 2$, $x^t = y^{t-1}$. For example, if we use \"imad\" as the input, y = [2, 3, 0, 1] while x = [$\\vec{0}$, 2, 3, 0]. Notice that $x^1 = \\vec{0}$ and not the index 0. \n",
    "- Compute the hidden state using the following formula:\n",
    "$$h^t = tanh(W_{hh}h^{t-1} + W_{xh}x^t + b)\\tag{1}\\\\{}$$\n",
    "Notice that we use hyperbolic tangent $(\\frac{e^x - e^{-x}}{e^x + e^{-x}})$ as the non-linear function. One of the main advantages of the hyperbolic tangent function is that it resembles the identity function.\n",
    "- Compute the output layer using the following formula:\n",
    "$$o^t = W_{hy}h^{t} + c\\tag{2}\\\\{}$$\n",
    "- Pass the output through softmax layer to normalize the output that allows us to express it as a probability, i.e. all output will be between 0 and 1 and sum up to 1. Below is the softmax formula:\n",
    "$$y^t = \\frac{e^{o^t}}{\\sum_ie^{o^t}}\\tag{3}\\\\{}$$\n",
    "The softmax layer has the same dimension as the output layer which is vocab_size x 1. As a result, $y^t[i]$ is the probability of of index $i$ being the next character.\n",
    "- As mentioned before, the objective of a character-level language model is to minimize the negative log-likelihood of the training sequence. Therefore, the loss function at time (t) and the total loss across all time steps are:\n",
    "$$\\mathcal{L}^t = -\\sum_{i = 1}^{T_y}y^tlog\\widehat{y^t}\\tag{4}\\\\{}$$\n",
    "$$\\mathcal{L} = \\sum_{t = 1}^{T_y}\\mathcal{L}^t\\tag{5}$$\n",
    "Since we'll be using SGD, the loss will be so noisy, so it's a good practice to smooth out the loss using expected weighted average of the past 1000 examples.\n",
    "- Pass the target character $y^t$ as the next input $x^{t + 1}$ until we finish the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: Georgia; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Backpropagation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{o^t}\\mathcal{L} = \\widehat{y^t} - y^t\\tag{6}\\\\{}$$\n",
    "$$\\nabla_{W_{hy}}\\mathcal{L} = \\nabla_{o^t}\\mathcal{L} . {h^t}^T\\tag{7}\\\\{}$$\n",
    "$$\\nabla_{c}\\mathcal{L} = \\sum_{i = 1}^{T_y}\\nabla_{o^t}\\mathcal{L} \\tag{8}\\\\{}$$\n",
    "$$\\nabla_{h^t}\\mathcal{L} = W_{hy}^T . \\nabla_{o^t}\\mathcal{L} + W_{hy}^T . \\nabla_{o^{t + 1}}\\mathcal{L} * (1 - tanh(h^{t + 1}) ^ 2)\\tag{9}\\\\{}$$\n",
    "$$\\nabla_{h^{t - 1}}\\mathcal{L} = W_{hh}^T . \\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2)\\tag{10}\\\\{}$$\n",
    "$$\\nabla_{x^t}\\mathcal{L} = W_{xh}^T . \\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2)\\tag{11}\\\\{}$$\n",
    "$$\\nabla_{W_{hh}}\\mathcal{L} = \\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2) . {h^{t - 1}}^T\\tag{12}\\\\{}$$\n",
    "$$\\nabla_{W_{xh}}\\mathcal{L} = \\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2) . {x^t}^T\\tag{13}\\\\{}$$\n",
    "$$\\nabla_{b}\\mathcal{L} = \\sum_{i = 1}^{T_x}\\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2) \\tag{14}\\\\{}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: Georgia; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Sampling</h3><br>\n",
    "Sampling is what makes the text generated by the RNN at each time step an interesting/creative text. On each time step (t), the RNN output the conditional probability distribution of the next character given all the previous characters, i.e. $P(c_t/c_1, c_2, ..., c_{t-a})$. Let's assume that at time step $t = 3$ where we're trying to predict the third character, the conditional probability distribution is: $P(c_3/c_1, c_2) = (0.2, 0.3, 0.4, 0.1)$. We'll have two extremes:\n",
    "1. Maximum entropy: the character will be picked randomly using uniform probability distribution; which means that all characters in the vocabulary dictionary are equally likely. Therefore, we'll end up with maximum randomness in picking the next characters and the generated text will not be either meaningful or sound real.\n",
    "2. Minimum entropy: the character with the highest conditional probability will be picked on each time step. That means next character will be what the model estimates to be the right one based on the training text and learned parameters. As a result, the name generated will be both meaningful and sound real.\n",
    "\n",
    "As we increase randomness, text will loose local structure; however, as we decrease randomness, the generated text will sound more real and start to preserve its local structure. For this exercise, we will sample from the distribution that's generated by the model which can be seen as an intermediate level of randomness between maximum and minimum entropy. Using this sampling strategy on the above distribution, the index 0 (letter \"a\" in our example) has $20%$ probability of being picked, while index 2 (letter \"i\" in our example) has $40%$ probability to be picked.\n",
    "<p align=\"left\">\n",
    "<img src=\"posts_images/char_level_model/sampling.png\"; style=\"width: 800px; height: 600px\"><br>\n",
    "<caption><center><u><font color=\"purple\">**Figure 2:**</font></u> Sampling: An example of predicting next character using character-level language model</center></caption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36121 characters and 26 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# Load names\n",
    "data = open(\"../data/names.txt\", \"r\").read()\n",
    "\n",
    "# Convert characters to lower case\n",
    "data = data.lower()\n",
    "\n",
    "# Construct vocabulary using unique characters, sort it in ascending order,\n",
    "# then construct two dictionaries that maps character to index and index to\n",
    "# characters.\n",
    "chars = list(sorted(set(data)))[1:]\n",
    "chars_to_idx = {ch:i for i, ch in enumerate(chars)}\n",
    "idx_to_chars = {i:ch for ch, i in chars_to_idx.items()}\n",
    "\n",
    "# Get the size of the data and vocab size\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars_to_idx)\n",
    "print(f\"There are {data_size} characters and {vocab_size} unique characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def model(file_path, chars_to_idx, idx_to_chars, hidden_layer_size, vocab_size, num_epochs=10, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Implements vanilla RNN to generate characters.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    file_path : str\n",
    "        path to the file of the raw data.\n",
    "    num_epochs : int\n",
    "        number of passes the optimization algorithm to go over the training data.\n",
    "    learning_rate : float\n",
    "        step size of learning.\n",
    "    chars_to_idx : python dict\n",
    "        dictionary mapping characters to indices.\n",
    "    idx_to_chars : python dict\n",
    "        dictionary mapping indices to characters.\n",
    "    hidden_layer_size : int\n",
    "        number of hidden units in the hidden layer.\n",
    "    vocab_size : int\n",
    "        size of vocabulary dictionary (unique characters in the data).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    # Get the data\n",
    "    with open(file_path) as f:\n",
    "        data = f.readlines()\n",
    "    examples = [x.lower().strip() for x in data]\n",
    "#     print(examples)\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n",
    "    \n",
    "    # Initialize Adam parameters\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 0\n",
    "\n",
    "    # Initialize loss\n",
    "    smoothed_loss = -np.log(1 / vocab_size) * 7\n",
    "    \n",
    "    # Initialize hidden state h0 and overall loss\n",
    "    h_prev = np.zeros((hidden_layer_size, 1))\n",
    "    overall_loss = []\n",
    "    \n",
    "    # Iterate over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}---\")\n",
    "        # Shuffle examples\n",
    "        np.random.shuffle(examples)\n",
    "        \n",
    "        counter = 0\n",
    "        # Iterate over all examples (SGD)\n",
    "        for example in examples:\n",
    "            y = [chars_to_idx[char] for char in example]\n",
    "            x = [None] + y[1:]\n",
    "            \n",
    "            # Fwd pass\n",
    "            loss, cache = rnn_forward(x, y, h_prev, parameters)\n",
    "            \n",
    "            # Compute smooth loss\n",
    "            smoothed_loss = smooth_loss(loss, smoothed_loss)\n",
    "\n",
    "            # Bwd pass\n",
    "            grads = rnn_backward(y, parameters, cache)\n",
    "            \n",
    "            # Update parameters\n",
    "            t += 1\n",
    "            parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate)\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        overall_loss.append(smoothed_loss)\n",
    "            # sample one name\n",
    "#             if counter % 1000 == 0:\n",
    "#                 print(sample(parameters, idx_to_chars, chars_to_idx, 7))\n",
    "        \n",
    "    return parameters, overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0---\n",
      "Epoch 1---\n",
      "Epoch 2---\n",
      "Epoch 3---\n",
      "Epoch 4---\n",
      "Epoch 5---\n",
      "Epoch 6---\n",
      "Epoch 7---\n",
      "Epoch 8---\n",
      "Epoch 9---\n"
     ]
    }
   ],
   "source": [
    "parameters, loss = model(\"../data/nam\", chars_to_idx, idx_to_chars, 100, vocab_size, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x119888128>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11840c2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(loss)), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Conclusion</h2><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More data, bigger model, and train longer leads to interesting results. With sampling technique we're using, don't expect the RNN to generate meaningful sequence of characters (names)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
