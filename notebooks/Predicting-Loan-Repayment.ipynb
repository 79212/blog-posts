{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: Georgia; font-size:3em;color:#2462C0; font-style:bold\">\n",
    "Predicting Loan Repayment</h1><br>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/Loans-borrow-repay.jpg\"; style=\"height: 400px; width: 800px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Introduction</h2><br>\n",
    "**Ensemble methods** can be defined as combining several different models (base learners) into final model (meta learner) to reduce the generalization error. It relies on the assumption that each model would look at a different aspect of the data which yield to capturing part of the truth. Combining good performing models the were trained independently will capture more of the truth than a single model. Therefore, this would result in more accurate predictions and lower generalization errors.\n",
    "- Almost always ensemble model performance gets improved as we add more models.\n",
    "- Try to combine models that are as much different as possible. This will reduce the correlation between the models that will improve the performance of the ensemble model that will lead to significantly outperform the best model. In the worst case where all models are perfectly correlated, the ensemble would have the same performance as the best model and sometimes even lower if some models are very bad. As aresult, pick models that are as good as possible.\n",
    "\n",
    "Diﬀerent ensemble methods construct the ensemble of models in diﬀerent ways. Below are the most common methods:\n",
    "- Averaging the predictions of all models.\n",
    "- Bagging: build different models on different datasets and then take the majority vote from all the models. Given the original dataset, we sample with replacement to get the same size of the original dataset. Therefore, each dataset will include, on average, 2/3 of the original data and the rest 1/3 will be duplicates. Since each model will be built on different dataset, it can be seen as a different model.\n",
    "- Boosting: build models sequentially. That means each model learns from the residuals for the previous model. The output will be all output of each single model weighted by the learning rate ($\\lambda$).\n",
    "- Build k models called base learners. Then fit a model to the output of the base learners to predict the final output. We'll explain more in \"Modeling\" section of how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Dataset</h2><br>\n",
    "The two most critical questions in the lending industry are: 1) How risky is the borrower? 2) Given the borrower's risk, should we lend him? The answer to the first question determines the interest rate the borrower would have. Interest rate measures among other things (such as time value of money) the riskness of the borrower. The riskier the borrower, the higher the interest rate. With interest rate in mind, we can then determine if the borrower is eligible for the loan.\n",
    "\n",
    "Investors(lenders) provide loans to borrowers in exchange for the promise of repayment with interest. That means the lender only makes profit (interest) if the borrower pays off the loan. However, if he doesn't repay the loan, then the lender loses money.\n",
    "\n",
    "We'll be using publicly available data from [LendingClub.com](https://www.LendingClub.com). The data covers the 9,578 loans funded by the platform between May 2007 and February 2010. The interest rate is provided to us for each borrower. Therefore, so we'll address the second question indirectly by trying to predict if the borrower will repay the loan by its mature date or not.\n",
    "\n",
    "Below is a short description of each feature in the data set:\n",
    "- credit_policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\n",
    "- purpose: The purpose of the loan such as: credit_card, debt_consolidation, etc.\n",
    "- int_rate: The interest rate of the loan (proportion).\n",
    "- installment: The monthly installments (\\$) owed by the borrower if the loan is funded.\n",
    "- log_annual_inc: The natural log of the annual income of the borrower.\n",
    "- dti: The debt-to-income ratio of the borrower.\n",
    "- fico: The FICO credit score of the borrower.\n",
    "- days_with_cr_line: The number of days the borrower has had a credit line.\n",
    "- revol_bal: The borrower's revolving balance.\n",
    "- revol_util: The borrower's revolving line utilization rate.\n",
    "- inq_last_6mths: The borrower's number of inquiries by creditors in the last 6 months.\n",
    "- delinq_2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n",
    "- pub_rec: The borrower's number of derogatory public records.\n",
    "- not_fully_paid: indicates whether the loan was not paid back in full (the borrower either defaulted or the borrower was deemed unlikely to pay it back).\n",
    "\n",
    "Let's load the data, take a look at the data types of each feature and check if we have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from keras import models, layers, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from warnings import filterwarnings\n",
    "import xgboost as xgb\n",
    "\n",
    "# os.chdir(\"../\")\n",
    "# from scripts.plot_roc import plot_conf_matrix_and_roc, plot_roc\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set_context(\"notebook\")\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94mData types:\n",
      "-----------\n",
      "\u001b[30mcredit_policy          int64\n",
      "purpose               object\n",
      "int_rate             float64\n",
      "installment          float64\n",
      "log_annual_inc       float64\n",
      "dti                  float64\n",
      "fico                   int64\n",
      "days_with_cr_line    float64\n",
      "revol_bal              int64\n",
      "revol_util           float64\n",
      "inq_last_6mths       float64\n",
      "delinq_2yrs          float64\n",
      "pub_rec              float64\n",
      "not_fully_paid         int64\n",
      "dtype: object\n",
      "\n",
      "\u001b[1m\u001b[94mSum of null values in each column:\n",
      "-----------------------------------\n",
      "\u001b[30mcredit_policy         0\n",
      "purpose               0\n",
      "int_rate              0\n",
      "installment           0\n",
      "log_annual_inc        4\n",
      "dti                   0\n",
      "fico                  0\n",
      "days_with_cr_line    29\n",
      "revol_bal             0\n",
      "revol_util           62\n",
      "inq_last_6mths       29\n",
      "delinq_2yrs          29\n",
      "pub_rec              29\n",
      "not_fully_paid        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_policy</th>\n",
       "      <th>purpose</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>log_annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>days_with_cr_line</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>not_fully_paid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>829.10</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>19.48</td>\n",
       "      <td>737</td>\n",
       "      <td>5639.958333</td>\n",
       "      <td>28854</td>\n",
       "      <td>52.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>228.22</td>\n",
       "      <td>11.082143</td>\n",
       "      <td>14.29</td>\n",
       "      <td>707</td>\n",
       "      <td>2760.000000</td>\n",
       "      <td>33623</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>366.86</td>\n",
       "      <td>10.373491</td>\n",
       "      <td>11.63</td>\n",
       "      <td>682</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>3511</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>162.34</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>8.10</td>\n",
       "      <td>712</td>\n",
       "      <td>2699.958333</td>\n",
       "      <td>33667</td>\n",
       "      <td>73.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>102.92</td>\n",
       "      <td>11.299732</td>\n",
       "      <td>14.97</td>\n",
       "      <td>667</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>4740</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit_policy             purpose  int_rate  installment  log_annual_inc  \\\n",
       "0              1  debt_consolidation    0.1189       829.10       11.350407   \n",
       "1              1         credit_card    0.1071       228.22       11.082143   \n",
       "2              1  debt_consolidation    0.1357       366.86       10.373491   \n",
       "3              1  debt_consolidation    0.1008       162.34       11.350407   \n",
       "4              1         credit_card    0.1426       102.92       11.299732   \n",
       "\n",
       "     dti  fico  days_with_cr_line  revol_bal  revol_util  inq_last_6mths  \\\n",
       "0  19.48   737        5639.958333      28854        52.1             0.0   \n",
       "1  14.29   707        2760.000000      33623        76.7             0.0   \n",
       "2  11.63   682        4710.000000       3511        25.6             1.0   \n",
       "3   8.10   712        2699.958333      33667        73.2             1.0   \n",
       "4  14.97   667        4066.000000       4740        39.5             0.0   \n",
       "\n",
       "   delinq_2yrs  pub_rec  not_fully_paid  \n",
       "0          0.0      0.0               0  \n",
       "1          0.0      0.0               0  \n",
       "2          0.0      0.0               0  \n",
       "3          0.0      0.0               0  \n",
       "4          1.0      0.0               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"../data/loans.csv\")\n",
    "\n",
    "# Check both the datatypes and if there is missing values\n",
    "print(f\"\\033[1m\\033[94mData types:\\n{11 * '-'}\")\n",
    "print(f\"\\033[30m{df.dtypes}\\n\")\n",
    "print(f\"\\033[1m\\033[94mSum of null values in each column:\\n{35 * '-'}\")\n",
    "print(f\"\\033[30m{df.isnull().sum()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have missing data in almost haf of the features. Let's check the count of each class to see if we are dealing with balanced/imbalanced classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive examples = 1533\n",
      "Negative examples = 8045\n",
      "Proportion of positive to negative examples = 19.06%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c23db70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get number of positve and negative examples\n",
    "pos = df[df[\"not_fully_paid\"] == 1].shape[0]\n",
    "neg = df[df[\"not_fully_paid\"] == 0].shape[0]\n",
    "print(f\"Positive examples = {pos}\")\n",
    "print(f\"Negative examples = {neg}\")\n",
    "print(f\"Proportion of positive to negative examples = {(pos / neg) * 100:.2f}%\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(df[\"not_fully_paid\"])\n",
    "plt.xticks((0, 1), [\"Not paid fully\", \"Paid fully\"])\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with imbalanced data, we'll explain in more details in \"Data Preprocessing\" section on how to deal with it. Note that positive example here means that the borrower didn't fully pay the loan, not the other way around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create dummy variables from the feature \"purpose\" since its nominal (not ordinal) categorical variable. It's also a good practice to drop the first one to avoid linear dependency between the resulted features since some algorithms may struggle with this issue.\n",
    "2. Split the data into train and test using 80/20 split rule.\n",
    "3. Impute the missing values using the mean of each feature.\n",
    "4. Standardize the data so that each feature would have zero mean and 1 standard deviation.\n",
    "5. Since we're dealing with imbalanced data, class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. To deal with this issue, we'll upsample the positive examples to be equal the negative examples and use metrics other than accuracy when comparing models such as *f1-score* or *AUC* (area under ROC curve).\n",
    "\n",
    "Note that **Imputation** and **Standardization** should be first fit to training data and then transform test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_policy</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>log_annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>days_with_cr_line</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>not_fully_paid</th>\n",
       "      <th>purpose_credit_card</th>\n",
       "      <th>purpose_debt_consolidation</th>\n",
       "      <th>purpose_educational</th>\n",
       "      <th>purpose_home_improvement</th>\n",
       "      <th>purpose_major_purchase</th>\n",
       "      <th>purpose_small_business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>829.10</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>19.48</td>\n",
       "      <td>737</td>\n",
       "      <td>5639.958333</td>\n",
       "      <td>28854</td>\n",
       "      <td>52.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>228.22</td>\n",
       "      <td>11.082143</td>\n",
       "      <td>14.29</td>\n",
       "      <td>707</td>\n",
       "      <td>2760.000000</td>\n",
       "      <td>33623</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>366.86</td>\n",
       "      <td>10.373491</td>\n",
       "      <td>11.63</td>\n",
       "      <td>682</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>3511</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>162.34</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>8.10</td>\n",
       "      <td>712</td>\n",
       "      <td>2699.958333</td>\n",
       "      <td>33667</td>\n",
       "      <td>73.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>102.92</td>\n",
       "      <td>11.299732</td>\n",
       "      <td>14.97</td>\n",
       "      <td>667</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>4740</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit_policy  int_rate  installment  log_annual_inc    dti  fico  \\\n",
       "0              1    0.1189       829.10       11.350407  19.48   737   \n",
       "1              1    0.1071       228.22       11.082143  14.29   707   \n",
       "2              1    0.1357       366.86       10.373491  11.63   682   \n",
       "3              1    0.1008       162.34       11.350407   8.10   712   \n",
       "4              1    0.1426       102.92       11.299732  14.97   667   \n",
       "\n",
       "   days_with_cr_line  revol_bal  revol_util  inq_last_6mths  delinq_2yrs  \\\n",
       "0        5639.958333      28854        52.1             0.0          0.0   \n",
       "1        2760.000000      33623        76.7             0.0          0.0   \n",
       "2        4710.000000       3511        25.6             1.0          0.0   \n",
       "3        2699.958333      33667        73.2             1.0          0.0   \n",
       "4        4066.000000       4740        39.5             0.0          1.0   \n",
       "\n",
       "   pub_rec  not_fully_paid  purpose_credit_card  purpose_debt_consolidation  \\\n",
       "0      0.0               0                    0                           1   \n",
       "1      0.0               0                    1                           0   \n",
       "2      0.0               0                    0                           1   \n",
       "3      0.0               0                    0                           1   \n",
       "4      0.0               0                    1                           0   \n",
       "\n",
       "   purpose_educational  purpose_home_improvement  purpose_major_purchase  \\\n",
       "0                    0                         0                       0   \n",
       "1                    0                         0                       0   \n",
       "2                    0                         0                       0   \n",
       "3                    0                         0                       0   \n",
       "4                    0                         0                       0   \n",
       "\n",
       "   purpose_small_business  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dummy variables from the feature purpose\n",
    "df = pd.get_dummies(df, columns=[\"purpose\"], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7662, 18), (7662,), (1916, 18), (1916,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dataframe into features and labels nd-arrays\n",
    "X = df.loc[:, df.columns != \"not_fully_paid\"].values\n",
    "y = df.loc[:, df.columns == \"not_fully_paid\"].values.flatten()\n",
    "\n",
    "# Split the data into train and test using 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=123, stratify=y)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94mSum of null values in training data: 0\n",
      "Sum of null values in test data: 0\n"
     ]
    }
   ],
   "source": [
    "# Impute the missing data using features means\n",
    "imp = Imputer()\n",
    "imp.fit(X_train)\n",
    "X_train = imp.transform(X_train)\n",
    "X_test = imp.transform(X_test)\n",
    "print(f\"\\033[1m\\033[94mSum of null values in training data: {np.isnan(X_train).sum()}\")\n",
    "print(f\"Sum of null values in test data: {np.isnan(X_test).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "std = StandardScaler()\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train) \n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (7662, 18) (7662,)\n",
      "Upsampled shape: (12872, 18) (12872,)\n"
     ]
    }
   ],
   "source": [
    "# Upsample minority class\n",
    "X_train_u, y_train_u = resample(X_train[y_train == 1],\n",
    "                                y_train[y_train == 1],\n",
    "                                replace=True,\n",
    "                                n_samples=X_train[y_train == 0].shape[0],\n",
    "                                random_state=1)\n",
    "X_train_u = np.concatenate((X_train[y_train == 0], X_train_u))\n",
    "y_train_u = np.concatenate((y_train[y_train == 0], y_train_u))\n",
    "print(\"Original shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Upsampled shape:\", X_train_u.shape, y_train_u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (7662, 18) (7662,)\n",
      "Downsampled shape: (2452, 18) (2452,)\n"
     ]
    }
   ],
   "source": [
    "# Upsample minority class\n",
    "X_train_d, y_train_d = resample(X_train[y_train == 0],\n",
    "                                y_train[y_train == 0],\n",
    "                                replace=True,\n",
    "                                n_samples=X_train[y_train == 1].shape[0],\n",
    "                                random_state=1)\n",
    "X_train_d = np.concatenate((X_train[y_train == 1], X_train_d))\n",
    "y_train_d = np.concatenate((y_train[y_train == 1], y_train_d))\n",
    "print(\"Original shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Downsampled shape:\", X_train_d.shape, y_train_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is ready for modeling. Also, the upsampled training data size is 12,872; however, the test data size is 1,916 which was not affected by the upsampling step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Modeling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.47\n"
     ]
    }
   ],
   "source": [
    "pip_rf = make_pipeline(RandomForestClassifier(class_weight=\"balanced\",\n",
    "                                              random_state=123))\n",
    "hyperparam_grid = {\n",
    "        \"randomforestclassifier__n_estimators\": [10, 50, 100, 500],\n",
    "        \"randomforestclassifier__max_features\": [\"sqrt\", \"log2\", 0.4, 0.5],\n",
    "        \"randomforestclassifier__min_samples_leaf\": [1, 3, 5],\n",
    "        \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"]}\n",
    "    \n",
    "gs_rf = GridSearchCV(pip_rf,\n",
    "                     hyperparam_grid,\n",
    "                     scoring=\"f1\",\n",
    "                     cv=10,\n",
    "                     n_jobs=-1)\n",
    "gs_rf.fit(X_train_u, y_train_u)\n",
    "print(f\"{gs_rf.best_score_ * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6817423572210874"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, gs_rf.best_estimator_.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.74\n"
     ]
    }
   ],
   "source": [
    "pip_logmod = make_pipeline(LogisticRegression(class_weight=\"balanced\"))\n",
    "\n",
    "hyperparam_range = np.arange(0.5, 20.1, 0.5)\n",
    "\n",
    "hyperparam_grid = {\"logisticregression__penalty\": [\"l1\", \"l2\"],\n",
    "                   \"logisticregression__C\":  hyperparam_range,\n",
    "                   \"logisticregression__fit_intercept\": [True, False]\n",
    "                  }\n",
    "\n",
    "gs_logmodel = GridSearchCV(pip_logmod,\n",
    "                           hyperparam_grid,\n",
    "                           scoring=\"f1\",\n",
    "                           cv=10,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "gs_logmodel.fit(X_train_u, y_train_u)\n",
    "print(f\"{gs_logmodel.best_score_ * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7061217135696398"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, gs_logmodel.best_estimator_.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 27104 candidates, totalling 54208 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    2.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1511ec8649e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                       verbose=1)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgs_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{gs_xgb.best_score_ * 100:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pip_xgb = make_pipeline(xgb.XGBClassifier())\n",
    " \n",
    "hyperparam_grid = {\n",
    "    \"xgbclassifier__n_estimators\": [10, 50, 100, 500],\n",
    "    \"xgbclassifier__learning_rate\": [0.01, 0.1, 0.5, 1.0, 10.0, 50.0, 100.0],\n",
    "    \"xgbclassifier__gamma\": np.arange(0., 0.51, 0.05),\n",
    "    \"xgbclassifier__max_depth\": [1, 2, 3, 4, 5, 10, 20, 50],\n",
    "    \"xgbclassifier__subsample\": np.arange(0.0, 1.01, 0.1)\n",
    "                  }\n",
    "\n",
    "gs_xgb = GridSearchCV(pip_xgb,\n",
    "                      hyperparam_grid,\n",
    "                      scoring=\"f1\",\n",
    "                      cv=2,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=1)\n",
    "\n",
    "gs_xgb.fit(X_train_u, y_train_u)\n",
    "print(f\"{gs_xgb.best_score_ * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0,
     11,
     32
    ]
   },
   "outputs": [],
   "source": [
    "# Build and train ensemble model\n",
    "def train_ensemble(X_train, y_train, X_test):\n",
    "    # Split the training data into training and validation where training\n",
    "    # will be used in fitting base learners and validation will be used to\n",
    "    # fit meta learner model,=.\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,\n",
    "                                                          test_size=0.5,\n",
    "                                                          random_state=123,\n",
    "                                                          stratify=y)\n",
    "\n",
    "    # Build base learners\n",
    "    def build_base_learners():\n",
    "        base_learners = {\"random_forest\": RandomForestClassifier(n_estimators=500,\n",
    "                                                                 max_features=0.25,\n",
    "                                                                 criterion=\"entropy\"),\n",
    "                         \"logistic_regression\": LogisticRegression(C=1.5,\n",
    "                                                                   penalty=\"l1\",\n",
    "                                                                   fit_intercept=True),\n",
    "                         \"extra_tree\": ExtraTreesClassifier(n_estimators=1000,\n",
    "                                                            max_features=\"log2\",\n",
    "                                                            criterion=\"entropy\"),\n",
    "                         \"gradient_boosting\": GradientBoostingClassifier(loss=\"deviance\",\n",
    "                                                                         learning_rate=0.1,\n",
    "                                                                         n_estimators=500,\n",
    "                                                                         max_depth=3,\n",
    "                                                                         max_features=\"log2\"),\n",
    "                         \"svc\": SVC(C=0.01, gamma=0.1, kernel=\"poly\", degree=3, coef0=10,\n",
    "                                    probability=True)}\n",
    "        return base_learners\n",
    "\n",
    "\n",
    "    # Build meta learner\n",
    "    def build_meta_learner():\n",
    "        meta_learner = RandomForestClassifier(n_estimators=500,\n",
    "                                              max_features=0.25,\n",
    "                                              criterion=\"entropy\")\n",
    "        return meta_learner\n",
    "\n",
    "\n",
    "    # Train base learners\n",
    "    def train_base_learners(base_learners_dict, X_train, y_train):\n",
    "        for name, classifier in base_learners.items():\n",
    "            print(f\"\\033[1m\\033[94mTraining {name} classifier...\")\n",
    "            classifier.fit(X_train, y_train)\n",
    "            print(f\"\\033[1m\\033[92mDone\")\n",
    "        \n",
    "\n",
    "    # Predict the prob matrix for positive examples for all base learners\n",
    "    def base_learners_predict(base_learners, X_valid):\n",
    "        base_learners_names = base_learners.keys()\n",
    "        prob_matrix = np.zeros((X_valid.shape[0], len(base_learners)))\n",
    "        prob_matrix = pd.DataFrame(prob_matrix)\n",
    "        prob_matrix.columns = base_learners_names\n",
    "\n",
    "        for name, classifier in base_learners.items():\n",
    "            prob_matrix.loc[:, name] = classifier.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        return prob_matrix\n",
    "\n",
    "\n",
    "    # Train meta learner using the probability matrix from base learners prediction\n",
    "    # and the validation labels\n",
    "    def train_meta_learner(meta_learner, base_learners_probs, y_valid):\n",
    "            print(f\"\\033[1m\\033[94mTraining starts...\")\n",
    "            meta_learner.fit(base_learners_probs, y_valid)\n",
    "            print(f\"\\033[1m\\033[92mDone\")\n",
    "\n",
    "\n",
    "    # Predict test labels using meta learner\n",
    "    def meta_learner_predict(base_learners, meta_learner, X_test):\n",
    "        base_learner_probs = base_learners_predict(base_learners, X_test)\n",
    "        return meta_learner.predict_proba(base_learner_probs)[:, 1]\n",
    "    \n",
    "    return meta_learner_predict(base_learners, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    \"\"\"Implement ensemble model\"\"\"\n",
    "    def __init__(self, base_learners, X_train, y_train):\n",
    "        \"\"\"\n",
    "        base_learners: dict\n",
    "            base learners sklearn.estimator\n",
    "        \"\"\"\n",
    "        self.base_learners = base_learners\n",
    "        self.meta_learner = RandomForestClassifier(n_estimators=500,\n",
    "                                                   max_features=0.25,\n",
    "                                                   criterion=\"entropy\")\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_train_base, self.y_train_base, self.X_train_pred, self.y_train_pred = split_train_data()\n",
    "        \n",
    "        def split_train_data(self):\n",
    "            \"\"\"Split the training data 50/50 into training for base learners and training\n",
    "               for meta learner.\n",
    "            \"\"\"\n",
    "            X_train_base, X_train_pred, y_train_base, y_train_pred = train_test_split(\n",
    "                self.X_train, self.y_train, test_size=0.5, random_state=123, stratify=y)\n",
    "            \n",
    "            return X_train_base, y_train_base, X_train_pred, y_train_pred\n",
    "        \n",
    "        def train_base_learner(self):\n",
    "            \"\"\"Train base learners\"\"\"\n",
    "            for name, classifier in self.base_learners.items():\n",
    "                print(f\"\\033[1m\\033[94mTraining {name} classifier...\")\n",
    "                classifier.fit(X_train, y_train)\n",
    "                print(f\"\\033[1m\\033[92mDone\")\n",
    "                \n",
    "        def base_learners_predict(self):\n",
    "            self.prob_matrix = np.zeros((self.X_train_pred.shape[0], len(self.base_learners)))\n",
    "            self.prob_matrix = pd.DataFrame(prob_matrix)\n",
    "            self.prob_matrix.columns = base_learners.keys()\n",
    "\n",
    "            for name, classifier in self.base_learners.items():\n",
    "                self.prob_matrix.loc[:, name] = classifier.predict_proba(self.X_train_pred)[:, 1]\n",
    "\n",
    "            return self.prob_matrix\n",
    "        \n",
    "        def train_meta_learner(self)\n",
    "            print(f\"\\033[1m\\033[94mTraining starts...\")\n",
    "            meta_learner.fit(self.prob_matrix, y_train_pred)\n",
    "            print(f\"\\033[1m\\033[92mDone\")\n",
    "            \n",
    "        def meta_learner_predict(self, X_test):\n",
    "            base_learner_pred = \n",
    "            return meta_learner.predict_proba(base_learner_probs)[:, 1]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9578, 19)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base learners\n",
    "base_learners = {\"random_forest\": RandomForestClassifier(n_estimators=500,\n",
    "                                                         max_features=0.25,\n",
    "                                                         criterion=\"entropy\"),\n",
    "                 \"logistic_regression\": LogisticRegression(C=1.5,\n",
    "                                                           penalty=\"l1\",\n",
    "                                                           fit_intercept=True),\n",
    "#                  \"extra_tree\": ExtraTreesClassifier(n_estimators=1000,\n",
    "#                                                     max_features=\"log2\",\n",
    "#                                                     criterion=\"entropy\"),\n",
    "#                  \"gradient_boosting\": GradientBoostingClassifier(loss=\"deviance\",\n",
    "#                                                                  learning_rate=0.1,\n",
    "#                                                                  n_estimators=500,\n",
    "#                                                                  max_depth=3,\n",
    "#                                                                  max_features=\"log2\"),\n",
    "                 \"svc\": SVC(C=0.01, gamma=0.1, kernel=\"poly\", degree=3, coef0=10,\n",
    "                            probability=True)}\n",
    "\n",
    "# Define meta learner\n",
    "# meta_learner = RandomForestClassifier(n_estimators=500,\n",
    "#                                       max_features=0.25,\n",
    "#                                       criterion=\"entropy\")\n",
    "# meta_learner = LinearRegression()\n",
    "meta_learner = GradientBoostingClassifier(loss=\"deviance\",\n",
    "                                         learning_rate=0.01,\n",
    "                                         n_estimators=500,\n",
    "                                         max_depth=3,\n",
    "                                         max_features=\"log2\")\n",
    "meta_learner = LogisticRegression(C=1.5,\n",
    "                                   penalty=\"l1\",\n",
    "                                   fit_intercept=True)\n",
    "\n",
    "def train_base_learners(base_learners, X_train_base, y_train_base):\n",
    "    print(\"Training base learners starts\")\n",
    "    print(20 * \"---\")\n",
    "    for name, classifier in base_learners.items():\n",
    "        print(f\"\\033[1m\\033[94mTraining {name} classifier...\")\n",
    "        classifier.fit(X_train_base, y_train_base)\n",
    "        print(f\"\\033[1m\\033[92mDone\")\n",
    "    print(20 * \"---\")\n",
    "\n",
    "\n",
    "def predict_base_learners(base_learners, X_train_pred):\n",
    "    probs_matrix = np.zeros((X_train_pred.shape[0], len(base_learners)))\n",
    "    probs_matrix = pd.DataFrame(probs_matrix)\n",
    "    probs_matrix.columns = base_learners.keys()\n",
    "\n",
    "    for name, classifier in base_learners.items():\n",
    "        probs_matrix.loc[:, name] = classifier.predict_proba(X_train_pred)[:, 1]\n",
    "\n",
    "    return probs_matrix\n",
    "\n",
    "\n",
    "\n",
    "def train_meta_learner(meta_learner, base_learners_probs, y_train_pred):\n",
    "    print(f\"\\033[1m\\033[94mTraining meta learner starts...\")\n",
    "    meta_learner.fit(base_learners_probs, y_train_pred)\n",
    "    print(f\"\\033[1m\\033[92mDone\")\n",
    "    print(20 * \"---\")\n",
    "    \n",
    "\n",
    "def predict_meta_learner(meta_learner, base_learner_probs, X_test):\n",
    "    return meta_learner.predict_proba(base_learner_probs)[:, 1]\n",
    "#     return meta_learner.predict(base_learner_probs)\n",
    "\n",
    "\n",
    "def ensemble_avg(probs_matrix):\n",
    "    return probs_matrix.mean(axis=1)\n",
    "\n",
    "\n",
    "# Define ensemble weighing method\n",
    "def plot_ensemble_roc(base_learners, meta_learner, X_train, y_train, X_test, y_test):\n",
    "   # Split the data 50/50 into training for base learners and training for meta learner\n",
    "    X_train_base, X_train_pred, y_train_base, y_train_pred = train_test_split(\n",
    "        X_train_u, y_train_u, test_size=0.4, random_state=123, stratify=y_train_u) \n",
    "    \n",
    "    # Train both base learners and meta learner\n",
    "    # Train base learners\n",
    "    train_base_learners(base_learners, X_train_base, y_train_base)\n",
    "    \n",
    "    # Predict probs matrix\n",
    "    base_probs = predict_base_learners(base_learners, X_train_pred)\n",
    "    \n",
    "    # Train meta learner\n",
    "    train_meta_learner(meta_learner, base_probs, y_train_pred)\n",
    "    \n",
    "    # Predict test examples\n",
    "    # Predict test probs\n",
    "    base_probs_test = predict_base_learners(base_learners, X_test)\n",
    "    \n",
    "    # Predict final output using meta learner estimator\n",
    "    meta_probs_test = predict_meta_learner(meta_learner, base_probs_test, X_test)\n",
    "    \n",
    "    # Avergae base learners\n",
    "    avg_probs_test = ensemble_avg(base_probs_test)\n",
    "    \n",
    "#     results = {}\n",
    "    \n",
    "#     for learner in estimators.keys():\n",
    "#         # Compute tpr, fpr, auc and confusion matrix\n",
    "#         fpr, tpr, thresholds = roc_curve(y, estimators[estimator].predict_proba(X)[:, 1])\n",
    "#         auc = roc_auc_score(y, estimators[estimator].predict_proba(X)[:, 1])\n",
    "\n",
    "#         # Plot ROC curce\n",
    "#         plt.plot(fpr, tpr, label=\"{}: auc = {:.3f}\".format(estimator, auc))\n",
    "#         plt.title(\"ROC curve\", y=1, fontdict={\"fontsize\": 20})\n",
    "#         plt.legend(loc=\"lower right\", fontsize=\"medium\")\n",
    "    \n",
    "#     plt.plot([0, 1], [0, 1], \"--\")\n",
    "#     plt.xlabel(\"False positive rate\", fontdict={\"fontsize\": 16})\n",
    "#     plt.ylabel(\"True positive rate\", fontdict={\"fontsize\": 16});\n",
    "    return base_probs_test, avg_probs_test, meta_probs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base learners starts\n",
      "------------------------------------------------------------\n",
      "\u001b[1m\u001b[94mTraining random_forest classifier...\n",
      "\u001b[1m\u001b[92mDone\n",
      "\u001b[1m\u001b[94mTraining logistic_regression classifier...\n",
      "\u001b[1m\u001b[92mDone\n",
      "\u001b[1m\u001b[94mTraining svc classifier...\n",
      "\u001b[1m\u001b[92mDone\n",
      "------------------------------------------------------------\n",
      "\u001b[1m\u001b[94mTraining meta learner starts...\n",
      "\u001b[1m\u001b[92mDone\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "b, a, m = plot_ensemble_roc(base_learners, meta_learner, X_train_d, y_train_d, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7003864661928121, 0.6326000125515474)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, a), roc_auc_score(y_test, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a174fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(base_learners[\"random_forest\"].predict_proba(X_test)[:, 1][y_test == 1], bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1517,   92],\n",
       "       [ 258,   49]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, base_learners[\"random_forest\"].predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6852416071649091\n",
      "0.7039555594244913\n",
      "0.666343430580833\n"
     ]
    }
   ],
   "source": [
    "for _ in b.columns:\n",
    "    print(roc_auc_score(y_test, b[_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAIeCAYAAAAVqBMbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xl0FHW+//9XdxZCSMAskMguCShBEGIUiA4kgDpuV1SMI4qDcESHcbkqiiBcVAjIRXQUEGUE3Ib5srj8FHFnUQyrLCOENRA0JGQFkpCELF2/P7i2NoSkgLRdUs/HOX3OVHUt74p9nLevz6eqHIZhGAIAAIDPOH1dAAAAgN3RkAEAAPgYDRkAAICP0ZABAAD4GA0ZAACAj9GQAQAA+BgNGXCeW7lypRwOhwoKCs7pOJmZmXI4HNq4cWMDVWZNDfX3AoAzQUMGNKDc3Fw9+uijiomJUaNGjdSqVStdf/31WrZsma9LOyNJSUl66KGHPNa1adNGOTk56t69u1fP/UtD1KxZM5WVlXl8t2PHDjkcjjNumIYOHaqbbrrJ1LaJiYnKyclRRETEGdUNAOeChgxoIJmZmYqPj9cXX3yhKVOm6D//+Y++/vpr3XjjjXrwwQfP+rjV1dWq7fnNlZWV51LuGfPz81N0dLT8/f1/l/M1a9ZMixcv9lg3d+5ctW3b1mvnrKqqUmBgoKKjo+VwOLx2HgA4GQ0Z0EBGjhwpwzC0ceNGpaSk6OKLL1bnzp310EMPaevWre7tfvrpJ916660KDQ1VaGiobrvtNmVlZbm/f/bZZ3XppZfqrbfecidtx44dU1JSkv72t79p1KhRat68ua666ipJ0tGjRzVixAi1aNFCoaGh6tu3b53DioWFhbrrrrvUunVrNW7cWF26dNH8+fPd3w8dOlSrVq3SrFmz3GlUZmZmrUOW3377rXr27KmgoCBFRUXpscce82gUk5KSNHLkSI0dO1aRkZFq0aKFRo0aJZfLVe/fc+jQoZo3b557uaqqSu+++66GDh3qsV1NTY2GDx+uiy66SI0bN1bHjh31v//7v+5zPPvss3r77bf16aefuq9n5cqV7uv597//rX79+qlx48Z64403ThmyHD58uLp06aLy8nL3+a6++mrTiRsAmEFDBjSAoqIiff7553rooYcUEhJyyvdhYWGSJMMwNHDgQOXm5mr58uVasWKFsrOzNXDgQI8UbP/+/VqwYIEWL16srVu3KigoSJL03nvvyTAMfffdd3rnnXdkGIZuvPFGHTx4UEuXLtXmzZvVp08f9evXTzk5ObXWWlFRofj4eC1dulTbt2/Xo48+qgceeEDffPONJOmVV15R7969dd999yknJ0c5OTlq06bNKcc5ePCgrr/+evXo0UObN2/W3Llz9e9//1tjxozx2O5f//qX/P39lZaWppkzZ+of//iHFi5cWO/f9J577tH69euVkZEhSVq6dKlCQkKUlJTksZ3L5VKrVq20aNEi7dixQ6mpqZo8ebK7yRw1apRSUlI0YMAA9/UkJia69x8zZoxGjhyp9PR0DRw48JQ6Xn31VVVVVWnUqFGSpNTUVO3du9ejWQSAc2YAOGfr1q0zJBkffPBBndt9+eWXhtPpNPbv3+9el5GRYTgcDuOrr74yDMMwJkyYYPj7+xuHDh3y2Ldv375G165dPdZ98803RpMmTYyysjKP9ZdddpkxdepUwzAMY8WKFYYkIz8//7R13Xnnncbw4cM9zvX3v//dY5v9+/cbkowNGzYYhmEYY8eONWJiYoyamhr3NvPnzzcCAwONY8eOuY/Tq1cvj+MMGDDA41wn+229KSkpxtixYw3DMIwbb7zRmDhxoqnrGT16tNG/f3/38l//+lfjxhtvrPV6XnzxxdOe/xcbNmwwAgICjPHjxxv+/v7GsmXLTntuADgbJGRAAzBqmeNVmx07dqhly5Zq3769e12HDh3UsmVLpaenu9e1bt1aUVFRp+x/+eWXeyz/8MMPKisrU/PmzRUSEuL+bNu2zZ0snaympkapqanq1q2bIiIiFBISog8++EA//fSTqWv47bX07t1bTuev/xq5+uqrVVlZqb1797rXdevWzWO/li1bKi8vz9Q5hg8frrfffls///yzvvrqq1OGK3/x+uuvKyEhwf13ePnll01fT0JCgqltnnnmGU2cOFEjRozQ9ddfb+rYAGDW7zM7FzjPdezYUQ6HQzt27NCtt9562u0MwzjtZPHfrm/SpEmt25y83uVyKSoqSt99990p2zZt2rTWY7z44ouaPn26XnnlFXXt2lUhISEaO3as6SbpF2avJSAg4JTvzMwhk6QBAwbIz89P9957r/r166fWrVt7NHuStHDhQv33f/+3XnzxRSUmJqpp06aaNWuWPvzwQ1PnON3f+rcMw9Dq1avl5+enjIyMOq8dAM4GCRnQAMLDw3Xddddp5syZKi0tPeX7I0eOSJLi4uJ08OBBZWZmur/bt2+fsrOzFRcXd8bnjY+PV25urpxOp2JjYz0+LVq0qHWf1atX6+abb9aQIUPUvXt3xcTEaPfu3R7bBAYGqqamps5zx8XFac2aNR7N1erVqxUYGKiYmJgzvpbaOJ1ODR06VCtXrtTw4cNr3Wb16tXq2bOnHnroIcXHxys2NvaUdNDM9dTlpZde0qZNm/Ttt99q7dq1mjFjxlkfCwBqQ0MGNJDXXntNhmEoISFBixcv1q5du7Rz507Nnj3bPWw3YMAAXXbZZbr77rv1ww8/aOPGjbr77rsVHx+vfv36nfE5BwwYoKuuukq33HKLPvvsM+3fv19r1qzRhAkTak3NJKlTp0765ptvtHr1au3cuVMPPfSQ9u/f77FN+/bttX79emVmZqqgoKDWRGvkyJHKzs7WyJEjtWPHDn366ad6+umn9dBDDyk4OPiMr+V0xo0bp/z8fN12222nvZ5Nmzbps88+0549ezRx4kStWrXqlOvZtm2bdu3apYKCAlVVVZk+/9atW/XMM89ozpw5SkxM1OzZszV69Ght27btnK4LAH6LhgxoIBdddJE2bdqka665RqNHj1a3bt3Ur18/ffzxx3rjjTcknRiu++ijj9S8eXMlJSUpOTlZ0dHR+uijj85qCMzhcGjZsmXq16+f7r//fl188cVKSUnRrl271LJly1r3GTdunK688kpdf/316tOnj5o0aaK7777bY5tRo0YpMDBQcXFxat68ea3zsVq1aqXPPvtMmzdvVvfu3TVs2DDdddddmjx58hlfR10CAgIUGRnpMVfttx544AGlpKRo8ODBuuKKK5SZmaknnnjCY5v7779fnTt3ds8z+/77702du6KiQnfffbcGDx6s22+/XZJ01113adCgQbr77rt1/Pjxc7s4APg/DsPsbGQAAAB4BQkZAACAj3GXJQAAQC1ee+01bdq0Sc2aNdP06dNP+d4wDM2fP1+bN29Wo0aNNHLkSHXo0EHSiffyfvDBB5Kk22677ZSHWp+MhAwAAKAWSUlJGjt27Gm/37x5sw4dOqRXX31VI0aM0JtvvilJKi0t1ZIlSzR58mRNnjxZS5YsqfUO/N+iIQMAAKhFXFxcra/D+8XGjRvVp08fORwOderUSceOHdPhw4e1ZcsWdevWzf2w7m7dumnLli11nouGDAAA4CwUFRUpMjLSvRwREaGioiIVFRUpIiLCvT48PFxFRUV1Hos5ZAAAwBL2XH2dV4+/+ZHhWrJkiXt50KBBSklJOevj1fagCjNvMKmNTxqy7OxsX5wW8NCyZUt+i7CEX54Zx+8Rvna65xeeL1JSUs6pATtZRESECgoK3MuFhYUKCwtTeHi4x/uJi4qK6n0bC0OWAAAAZyEhIUHffvutDMPQ7t27FRwcrLCwMHXv3l1bt25VaWmpSktLtXXrVnXv3r3OYzFkCQAArMFhrZzoH//4h9LT01VSUqIHH3xQKSkpqq6uliRde+216tGjhzZt2qRHHnlEgYGBGjlypCQpJCREt99+u8aMGSPpxNBoXTcHSD56Uj+xPKyAIUtYBUOWsApfD1nu6XODV4/f8dtlXj3+ubBWKwoAAGBDDFkCAABLcDjrvhPxfEZCBgAA4GMkZAAAwBosNqn/92TfKwcAALAIEjIAAGAN9TzN/nxGQgYAAOBjJGQAAMAauMsSAAAAvkJCBgAALMHBHDIAAAD4CgkZAACwBqd9cyL7XjkAAIBFkJABAABrYA4ZAAAAfIWEDAAAWAMJGQAAAHyFhAwAAFiCg7ssAQAA4CskZAAAwBpsnJDRkAEAAGtgUj8AAAB8hYQMAABYAi8XBwAAgM+QkAEAAGtwkpABAADAR0jIAACANTjsmxPZ98oBAAAsgoQMAABYA3PIAAAA4CskZAAAwBJ4DhkAAAB8hoQMAABYA3dZAgAAwFdIyAAAgDVwlyUAAAB8hYQMAABYgsNp35zIvlcOAABgESRkAADAGngOGQAAAHyFhAwAAFgDCRkAAAB8hYQMAABYg43vsqQhAwAAlsDLxQEAAOAzJGQAAMAaeHUSAAAAfIWEDAAAWIPDvjmRfa8cAADAIkjIAACANXCXJQAAAHyFhAwAAFiCg7ssAQAA4CskZAAAwBqYQwYAAABfISEDAADWYOOXi9v3ygEAACyChAwAAFiCg4QMAAAAvkJCBgAArIG7LAEAAOArJGQAAMAaSMgAAADgKyRkAADAGmx8lyUNGQAAsAQHQ5YAAADwFRIyAABgDSRkAAAA8BUSMgAAYA1OEjIAAAD4CAkZAACwBod9cyL7XjkAAIBFkJABAABLcDCHDAAAAL5iuiFbs2aNqXUAAABnxen07sfCTA9ZfvTRR+rdu3e96wAAAM4HW7Zs0fz58+VyudS/f38NHDjQ4/v8/HzNnj1bxcXFCgkJ0cMPP6yIiAhJ0p133qm2bdtKkiIjIzV69Og6z1VvQ7Z582Zt3rxZRUVFmjdvnnt9eXm5nBbvNgEAwB+IhZ7U73K5NHfuXI0bN04REREaM2aMEhIS1Lp1a/c27777rvr06aOkpCRt27ZNCxYs0MMPPyxJCgwM1LRp00yfr96OKiwsTB06dFBAQIA6dOjg/iQkJOiZZ54xfaJFixYpJSVFKSkp+uyzz0zvBwAA8Hvbu3evoqOjFRUVJX9/fyUmJmrDhg0e22RlZalr166SpC5dumjjxo1nfb56E7L27durffv2uvrqq+Xvf2Lz0tJSFRYWKiQkxPSJfmnGJCk7O/ssywUAAOcrh4USsqKiIvfwoyRFRERoz549Htu0a9dO69at0w033KD169ervLxcJSUlCg0NVVVVlZ5++mn5+fnplltu0ZVXXlnn+UzPIZs0aZKeeuopuVwuPfnkk2ratKni4uL017/+9QwvEQAA4Pe3aNEiLVmyxL08aNAgd1h0MsMwTll3csM4ZMgQzZs3TytXrlTnzp0VHh4uPz8/SdJrr72m8PBw5ebm6vnnn1fbtm0VHR192tpMN2RlZWUKDg7WN998o+TkZKWkpGjUqFFmdwcAAKibl+em/3a0rj4REREqLCx0LxcWFiosLMxjm/DwcHcvVFFRoXXr1ik4ONj9nSRFRUUpLi5OmZmZdTZkpq+8pqZGhw8f1po1axQfH292NwAAgD+cmJgY5eTkKC8vT9XV1UpLS1NCQoLHNsXFxXK5XJKkDz/8UMnJyZJOTO2qqqpyb7Nr1y6PmwFqYzohGzRokFJTU3XxxRcrNjZWubm5dXZ6AAAAZ8RCc8j8/Pw0bNgwpaamyuVyKTk5WW3atNHChQsVExOjhIQEpaena8GCBXI4HOrcubOGDx8uSTp48KDmzJkjp9Mpl8ulgQMH1tuQOYzaBkm9jEn9sIKWLVvyW4QltGzZUhL/boTv/fJb9JWccZO8evwLJ43z6vHPhekhy+zsbD3//PN64oknJEkHDhzQ+++/77XCAACAzTgc3v1YmOmG7I033tDgwYPddw+0a9dOaWlpXisMAADYi8Pp9OrHykxXV1lZqdjYWM+dLX5xAAAAfwSmJ/WHhobq0KFD7mdwrF279pTbPwEAAM6axYcVvcl0QzZ8+HDNmTNHBw8e1AMPPKAWLVrokUce8WZtAAAAtmCqIXO5XMrIyND48eNVUVEhwzDUuHFjb9cGAADsxGnfhMzUJDCn06kvvvhCkhQUFEQzBgAA0IBMz8rv2rWrPv74YxUUFKi0tNT9AQAAaBA2fuyF6TlkK1askCR3UiadeMnmzJkzG74qAAAAGzHdkM2aNcubdQAAAJuz+rPCvMl0Q1ZdXa0vv/xSO3bskCR16dJFAwYMkL+/6UMAAACgFqZb0TfffFP79u3Tddddp+uuu0779u3Tm2++6c3aAACAnTic3v1YmOl4KyMjQ9OmTXMvX3rppXryySe9UhQAAICdmG4XnU6nDh065F7Ozc3l1UkAAKDhOB3e/ViY6YTsnnvu0XPPPaeoqCgZhqGCggL97W9/82ZtAAAAtlBvQ7ZmzRr17t1bUVFRevXVV5WdnS3DMNSqVSsFBAT8HjUCAAAbcFj8WWHeVO+Y40cffSRJmj59ugICAtSuXTu1b9+eZgwAAKCB1JuQhYSE6LnnnlNeXp6mTp16yvejR4/2SmEAAMBmLH4npDfV25CNGTNG+/bt08yZM3XzzTf/HjUBAADYSr0Nmb+/vzp16qRJkyapadOmp91u3rx5GjZsWIMWBwAAbMTid0J6k+lssK5mTJJ27dp1zsUAAADYEe89AgAA1mDjuyxpyAAAgCU4GLI8d4ZhNNShAAAAbKXBErIbbrihoQ4FAADsiMde1C8jI0MffPCBCgoKVFNTI8Mw5HA49OKLL0qSkpKSvFUjAADAec10Q/bqq69qyJAhatu2ra1fbQAAALzExv2F6YasadOmSkhI8GYtAAAAtmS6IUtJSdHrr7+uSy+91OM9lj179vRKYQAAwGZsfJel6YZsxYoVys7OVnV1tZzOXyfd0ZABAACcG9MN2YEDBzR9+nRv1gIAAGzM4bTvXZamr7xjx47KysryZi0AAAC2ZDoh27Vrl1atWqUWLVooICDglMdeAAAAnBOeQ1a/sWPHerMOAAAA2zLdkDVv3lyZmZnauXOnJOmSSy5R+/btvVUXAACwGxvfZWk6G1y2bJlmzJiho0eP6ujRo5oxY4Y+++wzb9YGAABgC6YTsuXLlys1NVVBQUGSpFtuuUXjxo3T9ddf77XiAACAfdj5TUCmEzLDMDyeP+Z0OmUYhleKAgAAsBPTCVlycrKeeeYZXXHFFZKkDRs2qF+/fl4rDAAA2IyNEzLTDdlNN92kuLg496T+kSNH6qKLLvJaYQAAAHZRb0NWWlrq/t8tWrRQixYtPL4LCQnxTmUAAMBebPyk/nobstGjR8vhcMgwDBUUFCgkJESGYejYsWOKjIzUrFmzfo86AQAAzlv1NmS/NFxz5sxRQkKC4uPjJUmbN2/Wjz/+6N3qAACAfdh4DpnpbDAjI8PdjElSjx49lJ6e7pWiAAAA7MT0pP6mTZvq/fff15/+9Cc5HA599913Cg0N9WZtAADARuz8HDLTDdmjjz6qxYsXu18m3rlzZz366KNeKwwAANgMk/rrFxISovvuu8+btQAAANiS6YYsOztbn3zyifLz81VTU+NeP2HCBK8UBgAAbIYhy/q9/PLLuuaaa9S/f3+PVygBAADg3JhuyJxOp6699lpv1gIAAOzMxoGP6Su//PLL9cUXX+jw4cMqLS11fwAAAHBuTCdkq1atkiR9/PHH7nUOh0MzZ85s+KoAAIDtOJzMIasXr0gCAADwDtMNmST99NNPysrKUlVVlXtd3759G7woAABgQ9xlWb/FixcrPT1dWVlZ6tGjhzZv3qxLLrmEhgwAAOAcmZ7Uv3btWo0fP14XXHCBRo4cqWnTpnkkZQAAAOfE4fTux8JMVxcYGCin0ymn06mysjI1a9ZMeXl53qwNAADAFkwNWRqGobZt2+rYsWPq37+/nn76aQUFBSk2Ntbb9QEAAJvgLst6OBwOZWZmqkmTJrr22mvVvXt3lZeXq127dt6uDwAA4LxnelJ/x44dtXfvXsXGxqpFixberAkAANgRd1nWb/v27fr666/VvHlzNWrUSIZhyOFw6MUXX/RmfQAAAOc90w3Z2LFjvVkHAACwO4vfCelNphuy5s2be7MOAAAA2zqjJ/UDAAB4jY3vsrRvNggAAGARJGQAAMASHNxlCQAA4GMMWQIAAMBXSMgAAIA1OO2bE9n3ygEAACyChAwAAFiDjR8Ma98rBwAAsAgSMgAAYAk89uJ31rJlS1+cFjgFv0VYCb9HwL5IyAAAgDXY+DlkPmnIxi38zBenBTxMuvN6ZWdn+7oMwJ2M8XuEr5HSetqyZYvmz58vl8ul/v37a+DAgR7f5+fna/bs2SouLlZISIgefvhhRURESJJWrlypDz74QJJ02223KSkpqc5zkZABAABrsNAcMpfLpblz52rcuHGKiIjQmDFjlJCQoNatW7u3effdd9WnTx8lJSVp27ZtWrBggR5++GGVlpZqyZIleuGFFyRJTz/9tBISEhQSEnLa83GXJQAAwEn27t2r6OhoRUVFyd/fX4mJidqwYYPHNllZWerataskqUuXLtq4caOkE8lat27dFBISopCQEHXr1k1btmyp83w0ZAAAwBocTu9+zkBRUZF7+FGSIiIiVFRU5LFNu3bttG7dOknS+vXrVV5erpKSklP2DQ8PP2XfkzFkCQAAbGHRokVasmSJe3nQoEFKSUmpdVvDME5Zd/JjOYYMGaJ58+Zp5cqV6ty5s8LDw+Xn51fr8ep7pAcNGQAAsASHl++yTElJOW0DdrKIiAgVFha6lwsLCxUWFuaxTXh4uEaNGiVJqqio0Lp16xQcHKzw8HClp6e7tysqKlJcXFyd52PIEgAA4CQxMTHKyclRXl6eqqurlZaWpoSEBI9tiouL5XK5JEkffvihkpOTJUndu3fX1q1bVVpaqtLSUm3dulXdu3ev83wkZAAAwBosdJeln5+fhg0bptTUVLlcLiUnJ6tNmzZauHChYmJilJCQoPT0dC1YsEAOh0OdO3fW8OHDJUkhISG6/fbbNWbMGEknhkbrusNSkhxGbYOkXsZzyGAFPIcMVsFzyGAVvn4OWclXK7x6/NBrkr16/HNBQgYAAKzBad+ZVPa9cgAAAIsgIQMAAJZQ36Mhzmc0ZAAAwBoYsgQAAICvkJABAABrsPGQJQkZAACAj5GQAQAAa/Dyq5OsjIQMAADAx0jIAACAJTgc9s2J7HvlAAAAFkFCBgAArIG7LAEAAOArJGQAAMAauMsSAAAAvkJCBgAArIG7LAEAAOArJGQAAMASHMwhAwAAgK+QkAEAAGvgOWQAAADwFRIyAABgDSRkAAAA8BUSMgAAYAkOp31zIhoyAABgDTZuyOx75QAAABZBQgYAAKyBSf0AAADwFRIyAABgDbw6CQAAAL5CQgYAACzB4bBvTmTfKwcAALAIEjIAAGAN3GUJAAAAXyEhAwAA1sBdlgAAAPAVEjIAAGANzCEDAACAr5CQAQAAS+A5ZAAAAPAZEjIAAGAN3GUJAAAAXyEhAwAA1uC0b05k3ysHAACwCBIyAABgCQ6eQwYAAABfISEDAADWYOM5ZDRkAADAGhiyBAAAgK+QkAEAAGsgIQMAAICvkJABAABLcPDqJAAAAPgKCRkAALAGh31zIvteOQAAgEWQkAEAAGvgLksAAAD4CgkZAACwBu6yBAAAgK+QkAEAAEtwcJclAAAAfIWEDAAAWANzyAAAAOArJGQAAMASyoMaefX4oV49+rkhIQMAAPAxGjIAAAAfY8jSQhoHBujWKy5VbHSkyo5X6cv/7NJ/fso5Zbt7+ySoXWSYe9nP6VRByTHN/GK1e13vju2U2Km9mgQF6khZhf713Q8qLC37Xa4DAACcGRoyC7k5Pk41LkMv/H/LdeEFTTXkT5fr0JES5RWXemz3zrcbPZaHJ1+pfblF7uXLO7TW5R1a653vflB+canCmwSrvKrqd7kGAABw5hiytIgAPz/FtY7W1z/uVmV1jQ4UHNbO7Dx1b9+yzv0uCG6sdpHh2nLgoCTJIalfl1gt27xT+f/XyBUdK1N5JQ0ZAABWRUJmEZGhTWQYhsewYs6REl3UPLzO/Xq0b6UDBUU6fKxcktQ0OEjNghsrqlmIbu/ZVS6Xoc2ZB7Vi+14ZXr0CAABwts6oIdu1a5fy8/NVU1PjXte3b98GL8qOAv39VFFV7bHueFWVGgX41blf9/YttTI9w73ctHGQJCk2OlIzPl+toIAADU26QsXlFdq4L6vhCwcAAOfMdEM2Y8YM5ebmqn379nI6fx3ppCFrGJXVNWoU4PmPo1GAv45X1ZxmD6ldZJhCghppe9Yh97rqGpck6bud+1VRVa2KqmptyPhJnS5sTkMGAIBFmW7I9u3bp5deekkOx9m91mDRokVasmSJJKnT7fed1THOZwUlx+R0OBQREuwetoy+oKnyiktOu0+P9q2UfjBXldW/Nm0FJaX/15QxQAkAwB+F6Un9bdq00ZEjR876RCkpKVq0aJEWLVp01sc4n1XV1Cj94CH1v7SjAvz81DbyAnVu2UJbMrNr3d7fz6kubaK1eX/WScdx6cefc3T1JR0U6O+npo2DlNChjXZl5/8elwEAAM6C6YSspKREjz/+uGJjY+Xv/+tuo0eP9kphdvTJD+m69YquGjOwn8qOV+njH7Yrr7hU7SLDdG+fBE384Cv3tnGtonS8qlr78opOOc7SH9J1yxVdNPq/+qmiqkobM7L0w36GKwEAsCqHYRimxrbS09NrXR8XF3fGJx238LMz3gdoaJPuvF7Z2bUnkMDvqWXLE4+34fcIX/vlt+grJSWnn6bTEEJDz+xtllu2bNH8+fPlcrnUv39/DRw40OP7goICzZo1S8eOHZPL5dLgwYMVHx+vvLw8PfbYY+6/Z8eOHTVixIg6z2U6IYuLi9ORI0eUkXHijr7Y2Fg1a9bsjC4MAADgj8Dlcmnu3LkaN26cIiIiNGbMGCUkJKh169bubd5//3317t1b1157rbKysjRlyhTFx8dLkqKjozVt2jTT5zPdkKWlpem9995zJ2Lz5s0w1EZKAAAgAElEQVTTkCFD1KtXL9MnAwAA+CPYu3evoqOjFRUVJUlKTEzUhg0bPBoyh8OhsrITN+KVlZUpLCys1mOZYboh+/DDDzVlyhR3KlZcXKyJEyfSkAEAgD+E3z7xQZIGDRqklJSUWrctKipSRESEezkiIkJ79uzx2OaOO+7QpEmT9Pnnn+v48eMaP368+7u8vDw99dRTaty4sf7yl7+oc+fOddZmuiFzuVweQ5QhISFyuVxmdwcAAPCplJSU0zZgJ6ttiv3Jj/76/vvvlZSUpJtvvlm7d+/WjBkzNH36dIWFhem1115TaGio9u3bp2nTpmn69OkKDg4+7flMN2Tdu3dXamqqrrrqKkknhjB79OhhdncAAIA/jIiICBUWFrqXCwsLTxmSXL58ucaOHStJ6tSpk6qqqlRSUqJmzZopICBAktShQwdFRUUpJydHMTExpz2f6eeQDRkyRP3799eBAweUmZmpAQMG6J577jmjiwMAAPgjiImJUU5OjvLy8lRdXa20tDQlJCR4bBMZGalt27ZJkrKyslRVVaWmTZuquLjYPYqYm5urnJwc91y00zmjd1n26tWLOWMAAMArqvwCfF2Cm5+fn4YNG6bU1FS5XC4lJyerTZs2WrhwoWJiYpSQkKB7771Xb7zxhj799FNJ0siRI+VwOJSenq5FixbJz89PTqdT999/v0JCQuo8X73PIRs/frwmTpyoe++912Ps1DAMORwOvf3222d8kTyHDFbAc8hgFTyHDFbh6+eQFZVVePX44cFBXj3+uag3IZs4caIk6Z133vF6MQAAwL7MPar+/GR6DtmhQ4dUVVUlSdq+fbuWLVumY8eOea0wAAAAuzDdkE2fPl1Op1OHDh3S66+/rry8PL366qverA0AANiIyzC8+rEy0w2Z0+mUn5+f1q9frxtuuEFDhw7V4cOHvVkbAACALZhuyPz8/LR69WqtWrVKl19+uSSppqbGa4UBAAB7MQzDqx8rM92QjRw5Urt379att96qFi1aKC8vT3/605+8WRsAALAROzdkpp9D1rp1aw0bNkySVFpaqvLycg0cONBrhQEAANiF6YTs2WefVVlZmUpLS/Xkk0/qtddeO6tnkAEAANSGSf0mlJWVKTg4WOvWrVNycrKmTp2qH3/80Zu1AQAA2ILphqympkaHDx/WmjVrFB8f782aAACADRmGdz9WZrohGzRokFJTUxUVFaXY2Fjl5uYqOjram7UBAADYgulJ/b1791bv3r3dy1FRURo1apRXigIAAPZj9Tshvcl0Qpadna3nn39eTzzxhCTpwIEDev/9971WGAAAgF2YbsjeeOMNDR48WH5+fpKkdu3aKS0tzWuFAQAAe3HJ8OrHykw3ZJWVlYqNjfXc2Wl6dwAAAJyG6TlkoaGhOnTokBwOhyRp7dq1CgsL81phAADAXuw8h8x0QzZ8+HDNmTNHBw8e1AMPPKAWLVrokUce8WZtAAAAtmCqIXO5XMrIyND48eNVUVEhwzDUuHFjb9cGAABsxOpP0/cmU5PAnE6nvvjiC0lSUFAQzRgAAEADMj1k2bVrV3388cdKTExUUFCQe31ISIhXCgMAAPbictk3ITPdkK1YsUKS3EmZJDkcDs2cObPhqwIAALAR0w3ZrFmzvFkHAACwORtPITPfkK1bt+6UdcHBwWrbtq2aNWvWoEUBAADYiemGbPny5dq9e7e6dOkiSUpPT1fHjh2Vk5OjQYMGqU+fPl4rEgAAnP94DpkJDodDL7/8si644AJJ0pEjR/Tmm29q8uTJmjBhAg0ZAADAWTLdkOXn57ubMUlq1qyZcnJyFBIS4n6/JQAAwNmy+vsmvcl0Q9a5c2e98MIL6tWrl6QTr07q3LmzKioq1KRJE68VCAAA7IEhSxOGDx+udevWaefOnZKkpKQk9ezZUw6HQxMmTPBagQAAAOe7M5pDFhMTo+DgYHXr1k3Hjx9XRUUFT+0HAAANws4JmalXJ0nS119/rZdeekn//Oc/JUlFRUWaNm2a1woDAACwC9MN2RdffKGJEye6E7ELL7xQR48e9VphAADAXlyGdz9WZrohCwgIkL//ryOcNTU1cjgcXikKAADATkzPIYuLi9MHH3ygyspK/ec//9EXX3yhyy+/3Ju1AQAAG2EOmQmDBw9W06ZN1bZtW3311Vfq0aOH/vKXv3izNgAAAFswlZC5XC7NnDlTjzzyiAYMGODtmgAAgA2RkNW3kdOpkpISVVdXe7seAAAA2zE9h6x58+YaP368Lr/8cgUFBbnX33TTTV4pDAAA2IvLxgmZ6YYsLCxMYWFhMgxD5eXl3qwJAADAVkw3ZHfccUed38+bN0/Dhg0754IAAIA92TkhM32XZX127drVUIcCAACwFdMJGQAAgDdxlyUAAAB8psESMjt3tQAA4Nwxh6wB3HDDDQ11KAAAAFsx3ZBNnDhRx44dcy+XlpYqNTXVvZyUlNSghQEAAHsxDO9+rMx0Q1ZSUqImTZq4l0NCQnT06FGvFAUAAGAnpueQORwOFRQUKDIyUpKUn58vh8PhtcIAAIC92Hk+uumG7K677tL48eMVFxcnSdqxY4dGjBjhtcIAAADswnRD1r17d02dOlV79uyRYRj661//qqZNm3qzNgAAYCN2vsuy3obs4MGDatWqlfbt2yfpxDstJamgoEAFBQXq0KGDdysEAAC2wJBlHZYuXaoHHnhA7777bq3fT5gwocGLAgAAsJN6G7IHHnhAkjRmzBgFBgZ6fFdZWemdqgAAgO3YOCAz/9iL8ePHm1oHAACAM1NvQnbkyBEVFRWpsrJS+/fvd4/vlpeX6/jx414vEAAA2AOT+uuwZcsWrVq1SoWFhXrnnXfc64OCgnTXXXd5tTgAAAA7qLchS0pKUlJSktauXatevXr9HjUBAAAbsvNdlqbnkBUVFamsrEyGYej111/X6NGjtXXrVm/WBgAAYAumG7IVK1YoODhYW7du1dGjR/W3v/1NCxYs8GZtAADARlyG4dWPlZluyH6JETdv3qzk5GS1b9/e1tEiAABAQzH96qQOHTpo0qRJysvL0+DBg1VeXs7LxQEAQIOxeorlTaYbsgcffFCZmZmKiopSo0aNVFJSopEjR3qzNgAAAFsw/S7LzMxMSVJubq63awIAADZk56lQvMsSAADAx0y/y5LGCwAAeBMJmQnr1q07ZV1wcLDatm2rZs2aNWhRAAAAdmK6IVu+fLl2796tLl26SJLS09PVsWNH5eTkaNCgQerTp4/XigQAAOc/l30DMvMNmcPh0Msvv6wLLrhA0omXjr/55puaPHmyJkyYQEMGAABwlkw3ZPn5+e5mTJKaNWumnJwchYSEyM/PzyvFAQAA+2AOmQmdO3fWCy+84H7B+Nq1a9W5c2dVVFSoSZMmXisQAADgfOcwTLajhmFo3bp12rlzpyTpkksuUc+ePXlaPwAAaBBf/me3V49/bbdOXj3+uTijOWSXXHKJ/P395XA4FBsbe9bNWNKzM89qP6AhrXz2IY1esNTXZQCaOvgmSVJ2draPK4HdtWzZ0qfnd4khy3qlpaXpvffeU1xcnCRp3rx5GjJkiHsIEwAAAGfHdEP24YcfasqUKe5njhUXF2vixIk0ZAAAoEHYeVK/0+yGLpfL4wGwISEhcrlcXikKAADATkwnZN27d1dqaqquuuoqSSeGMHv06OG1wgAAgL1Y7cGwW7Zs0fz58+VyudS/f38NHDjQ4/uCggLNmjVLx44dk8vl0uDBgxUfHy/pxMji8uXL5XQ6dd9996l79+51nst0QzZkyBCtXbtWu3btkmEYGjBggK688sqzuDwAAABrc7lcmjt3rsaNG6eIiAiNGTNGCQkJat26tXub999/X71799a1116rrKwsTZkyRfHx8crKylJaWppeeuklHT58WBMnTtQrr7wip/P0A5OmGzJJ6tWrF3PGAACAV7gsFJHt3btX0dHRioqKkiQlJiZqw4YNHg2Zw+FQWVmZJKmsrExhYWGSpA0bNigxMVEBAQFq0aKFoqOjtXfvXnXqdPrHbtTbkN177721Pt7CMAw5HA69/fbbZ3aFAAAAFldUVKSIiAj3ckREhPbs2eOxzR133KFJkybp888/1/HjxzV+/Hj3vh07dnRvFx4erqKiojrPV29D9s4775zRBQAAAJwNb99luWjRIi1ZssS9PGjQIKWkpJiu5eSA6vvvv1dSUpJuvvlm7d69WzNmzND06dPP6jrOaMgSAADgjyolJeW0DdjJIiIiVFhY6F4uLCx0D0n+Yvny5Ro7dqwkqVOnTqqqqlJJSckp+xYVFSk8PLzO85l+7AUAAIA3GYbh1c+ZiImJUU5OjvLy8lRdXa20tDQlJCR4bBMZGalt27ZJkrKyslRVVaWmTZsqISFBaWlpqqqqUl5ennJychQbG1vn+UjIAAAATuLn56dhw4YpNTVVLpdLycnJatOmjRYuXKiYmBglJCTo3nvv1RtvvKFPP/1UkjRy5Eg5HA61adNGvXv31uOPPy6n06nhw4fXeYelREMGAAAswmrvsoyPj3c/V+wXd955p/t/t27dWhMnTqx139tuu0233Xab6XMxZAkAAOBjJGQAAMASeJclAAAAfIaEDAAAWIKNAzISMgAAAF8jIQMAAJbgsnFERkIGAADgYyRkAADAEux8lyUNGQAAsAQ7N2QMWQIAAPgYCRkAALAEJvUDAADAZ0jIAACAJZCQAQAAwGdIyAAAgCVwlyUAAAB8hoQMAABYgsu+ARkJGQAAgK+RkAEAAEtgDhkAAAB8hoQMAABYAgkZAAAAfIaEDAAAWAJP6gcAAIDPkJABAABLsHFARkIGAADgayRkAADAErjLEgAAAD5DQgYAACzBzndZ0pABAABLYMgSAAAAPkNCBgAALMHOQ5YkZAAAAD5GQgYAACyBhAwAAAA+Q0IGAAAsgbssAQAA4DMkZAAAwBJsHJCRkAEAAPgaCRkAALAE7rIEAACAz5CQAQAAS+AuSwAAAPgMCRkAALAEEjIAAAD4DAkZAACwBO6yBAAAgM+QkAEAAEuwbz5GQgYAAOBzJGQAAMASmEMGAAAAnyEhAwAAlmDn55DRkAEAAEtwuezbkDFkCQAA4GMkZAAAwBLsPGRJQgYAAOBjJGQAAMASeOwFAAAAfIaEDAAAWIJ98zESMgAAAJ8jIQMAAJbAXZYAAADwGRIyCwlt3EhP/Vc/JcS01dGycv3zm7X65sfdp2w39e6b1a3dhe5lfz8//VxwRMNm/1uSNCy5p66+pIPaNQ/Tu99u1Fsr1/9u14DzQ+PAAA3qeZk6XRipY8cr9fmWndpyIPuU7YYlXan2zcPdy35Op/JLSvWPZd9Kkkb/Vz+FBjVy3zl1oOCw5q5Y9/tcBIA/HDvfZUlDZiH/fUNfVdW4dNuL8xQbHakpg29SxqECZeYXeWw3+l+feCz/Y+it2rQ/y718sOioXv/qe/1XwqW/S904/wxMuFQ1LpcmfvCVWoY11X19r1TOkWLlHi312G7eSc3+iP69lZFb4LHurVUbtPekdQAATwxZWkRQgL/6xMVo3op1Kq+s0o8/5Sht135de9nFde4XfUGoura9UF9u3ele98XWnVq/9yeVV1Z5u2ychwL8/HRpmwv15X92qbK6Rpn5h5V+MFc92reuc7+wJo11UfNwj/84AIAzYRiGVz9WRkJmEa0jLpDLZSir8Ih7XUZuoS5r17LO/a697BL9+FOODh0p8XaJsInmTZvIMAwVlBxzr8s5XKyLoiLq3C/+otban1+kw8fKPdb/JbGHHA4p+3Cxlm1OVw6/VQA4BQ2ZRTQODNCx48c91pVWHFdwo8A697vusov17rcbvVkabCbQ318VVZ7pakVVtRr51/2vi/iLWmv5tj0e6/5f2mYdPHxUDklXXXyRhif31ItLV6qiqrqhywZwHrDzHDLTQ5YTJ07UsWO//hdzaWmpUlNTvVKUHZVXVp3SfDVpFKiy45Wn3adr2wsVHhKsVekZ3i4PNlJZXa1GAQEe6xoF+Ot49embqPbNwxQa1Eg//pzjsf5AwWFV17hUVePSyvQMlVdW66IW4ac5CgDYl+mErKSkRE2aNHEvh4SE6OjRo6ZPtGjRIi1ZsuTEQlwf8xXaRFbhEfk5nWoV3kwHi078XWOiI0+Z0P9b1112ib7dsY+5YmhQ+cXH5HQ4FBHaRIX/N2x5YVhT5dYx1Hj5RW20LeuQKqtr6jm6IcnRcMUCOK/YOCAzn5A5HA4VFPx6p1R+fr4cDvP/Yk1JSdGiRYu0aNGiM6vQJiqqqvXdjgwNS+6poAB/XdomWlddfJG+3Lqr1u0D/f2U1CVWn2/Zccp3fk6nAv395HA45Od0KNDfT84z+GcFe6uqqdH2rBxd27WTAvz81C4yTF1aRWlzZu2T9f39nOra9kL9sO9nj/UXBAepXWSY/JwO+Tud6tO5g4IbBepAHf+RAQB2ZTohu+uuuzR+/HjFxcVJknbs2KERI0Z4rTA7evnTVRp9S399+ORwFZdX6OVPVykzv0hd216o/73nZl0/eY5726sv6aDSiuPavP/gKcd58r+S9efund3LQ/pcoRc++lqfb9l5yrZAbT7csE139LxM/3P7NSo7XqUPN/yo3KOlat88XMOSrtT/LP7cvW2X1tGqqKpSRm6hxzEaBfjr1iu6KiI0WFU1LuUcLta8letVRqIL4DSsfiekNzmMM7j64uJi7dmzR4ZhqFOnTmratOlZnTTp2ZlntR/QkFY++5BGL1jq6zIATR18kyQpO/vUh+8Cv6eWLeu+s9/bnvl/y7x6/NS/3ODV458L0wnZ1KlTddVVVykhIUFBQUHerAkAANiQne+yNN2Q3XzzzUpLS9OCBQsUGxurxMRExcfHKzCw7scyAAAAmEFDZkJcXJzi4uLkcrm0bds2ff3115o9e7befvttb9YHAABw3jujB8NWVlZq48aNSktL0/79+9W3b19v1QUAAGzGapP6t2zZovnz58vlcql///4aOHCgx/dvvfWWtm/fLulEj3T06FG99dZbkqQ777xTbdu2lSRFRkZq9OjRdZ7LdEP28ssva+/evbrsssv05z//WXFxcXI6eRUmAAA4/7hcLs2dO1fjxo1TRESExowZo4SEBLVu/et7fYcOHer+35999pn279/vXg4MDNS0adNMn890RxUfH69p06ZpxIgR2rlzp1566SWPEwMAAJwLK71cfO/evYqOjlZUVJT8/f2VmJioDRs2nHb777//XldfffVZX7vphuyTTz5RcHCwdu7cqR9//FF9+/bVP//5z7M+MQAAgFUVFRUpIiLCvRwREaGiotofbJ2fn6+8vDxdeuml7nVVVVV6+umn9cwzz2j9+vX1ns/0kOUvw5ObNm3SNddcoyuuuEKLFy82uzsAAECdXF6eQubxGkdJgwYNUkpKSq3b1paone4NRd9//7169erlMZXrtddeU3h4uHJzc/X888+rbdu2io6OPm1tphuy8PBwzZkzRz/++KNuueUWVVVVWW7yHQAAwOmkpKSctgE7WUREhAoLf30DSWFhocLCwmrdNi0tTcOHD/dYFx4eLkmKiopSXFycMjMz62zITA9ZPvbYY7rssss0duxYNWnSRKWlpbrnnnvM7g4AAFAnK80hi4mJUU5OjvLy8lRdXa20tDQlJCScsl12draOHTumTp06udeVlpaqqurEa+KKi4u1a9cuj5sBamM6IWvUqJF69uzpXg4LCzttpwgAAPBH5ufnp2HDhik1NVUul0vJyclq06aNFi5cqJiYGHdztnr1aiUmJnoMZx48eFBz5syR0+mUy+XSwIEDG64hAwAA8CarTYWKj49XfHy8x7o777zTY7m2IdCLL75Y06dPP6Nz8SAxAAAAHyMhAwAAlmDnd1mSkAEAAPgYCRkAALAEq80h+z2RkAEAAPgYCRkAALAEbz+p38pIyAAAAHyMhAwAAFiCy3D5ugSfISEDAADwMRIyAABgCTa+yZKGDAAAWAOPvQAAAIDPkJABAABL4NVJAAAA8BkSMgAAYAnMIQMAAIDPkJABAABLICEDAACAz5CQAQAAS+Dl4gAAAPAZEjIAAGAJzCEDAACAz5CQAQAAS3CJhAwAAAA+QkIGAAAsgTlkAAAA8BkSMgAAYAkuGz+IjIQMAADAx0jIAACAJTCHDAAAAD5DQgYAACzBxlPIaMgAAIA1MGQJAAAAnyEhAwAAlmDw6iQAAAD4CgkZAACwBBdzyAAAAOArJGQAAMASuMsSAAAAPkNCBgAALMHOD4YlIQMAAPAxEjIAAGAJzCEDAACAz5CQAQAASyAhAwAAgM+QkAEAAEvgSf0AAADwGRIyAABgCSRkAAAA8BkSMgAAYAncZQkAAACfISEDAACWYOOAjIQMAADA10jIAACAJdj5LksaMgAAYAlM6gcAAIDPkJABAABLsPOQJQkZAACAj5GQAQAAS2AOGQAAAHyGhAwAAFiCjQMyEjIAAABfIyEDAACWwF2WAAAA8BkSMgAAYAl2vsvSYdj56v/AFi1apJSUFF+XAfBbhGXwW8QfGUOWf1BLlizxdQmAJH6LsA5+i/gjoyEDAADwMRoyAAAAH6Mh+4MaNGiQr0sAJPFbhHXwW8QfGZP6AQAAfIyEDAAAwMdoyAAAAHyMhgwAAMDHaMgAAAB8jIYMAADAx2jIAAAAfIyGzIf+/ve/q7i42CvHrqqq0sSJE/Xkk08qLS3NK+fIzMzUpk2bvHJsuxoyZMhZ7/v6668rKyvrtN+vXLlSRUVFprf/Ixk3bpyvSwCAc+Lv6wL+qAzDkGEYcjqt2dPu379f1dXVmjZtmul9XC7XGV1PZmamMjIyFB8ffzYlooE9+OCDdX6/cuVKtWnTRuHh4aa2r09NTY38/PzO6Rhn+ps7nUmTJp3zMfDHUFFRoZdffllFRUVyuVy6+eabtWnTJj3++OOSpO3bt+uTTz7R008/rS1btujf//63XC6XQkND9T//8z8+rh44PRqyM5CXl6cpU6aoS5cu2r17t9q3b6+ffvpJlZWV6tWrl1JSUiSdSL769u2rH374QdXV1Xr88cfVqlUrlZSU6JVXXlFxcbFiY2P122fyLl26VCtWrJAk9evXTzfeeKPy8vI0efJkXXLJJdqzZ4/atWunpKQkLV68WEePHtUjjzyi2NjYU+o8evSoZsyYoeLiYj355JN64oknlJ+fr3fffVc1NTWKiYnR/fffr4CAAP39739XcnKytm7dqj//+c+KiYnR3LlzVVxcrEaNGumBBx5Qq1attGbNGi1ZskROp1PBwcEaP368Fi5cqMrKSu3cuVO33nqrEhMTf59/EDZgGIbee+89bdmyRZJ0++23KzExUS6XS/PmzVN6erpatGghwzCUnJysXr166dlnn9WQIUN00UUXafbs2dq3b58kKTk5WZGRkcrIyNCrr76qwMBApaamavLkyRoyZIhiYmJM/x/XokWLdPjwYeXn5ys0NFQPP/yw/vWvfyk9PV1VVVW67rrrdM0119RZ59n+5p577jn9/PPPeu2111RdXS3DMPTEE0/owgsv1JAhQ/Tuu++e9u+2fft2LV68WKGhofr555/VoUMHPfzww3I4HL/PP1A0mC1btigsLExjxoyRJJWVlWnhwoWqqKhQUFCQ0tLSlJiYqOLiYr3xxht67rnn1KJFC5WWlvq4cqAeBkzLzc01UlJSjF27dhmGYRglJSWGYRhGTU2NMWHCBCMzM9MwDMMYOXKksWzZMsMwDOPzzz83Zs+ebRiGYcydO9dYvHixYRiG8cMPPxh33HGHcfToUSMjI8N4/PHHjfLycqO8vNx47LHHjH379hm5ubnGnXfeaRw4cMCoqakxnnrqKWPWrFmGy+Uy1q9fb0ydOvW0tW7bts2YMmWKYRiGcfz4cePBBx80Dh48aBiGYcyYMcNYunSpu9aPPvrIvd9zzz1nZGdnG4ZhGLt37zaeffZZwzAM4/HHHzcKCwsNwzCM0tJSwzAMY8WKFcabb755Tn9TeLrnnnsMwzCMNWvWGM8//7xRU1NjHD582HjwwQeNoqIiY82aNcbkyZPd64cOHWqsWbPGMAzDmDBhgrF3714jIyPDeP75593H/OWf1y/f/+KX5aNHjxoPPvigkZubaxjGr7/r2ixcuNB46qmnjOPHjxuGYRhfffWVsWTJEsMwDKOystIYPXq0kZubW2ed5/Kbmzt3rvHtt98ahmEYVVVV7jrq+7tt27bNuPfee42CggKjpqbGGDt2rLHj/2/v/kKa6sMAjn9dbzbnkkHjRIN01dycWc01NssoCbqpuyyK6CYMkV10U0sYdBXRXwwvJEHqpujSiiAykCJDq8EsoraMqCT/dEITQ1s0d94LX8/7ir02y7fpy/O524/fb3vO2W+cx+c8B2OxmXw1Yo7o6enRgsGgdvnyZe3FixeapmlaY2Oj9uDBAy2ZTGo1NTXa6OioFolEtPr6+gxHK0T6pEI2Q1arFafTCUB7ezutra2MjY3x6dMn3r9/T0FBAQCBQACAlStX8vjxYwBisRhHjhwBwOv1kpubC0A8Hsfv92M0GgHw+/3EYjF8Ph+KopCfnw/A8uXLWbNmDVlZWeTn5/Px48e0Yu7t7UVRFGw2GwBbtmyhpaWFHTt2AOiVrUQiwcuXL6mrq9PXJpNJAFwuFw0NDWzYsEE/NvHficfjlJeXYzAYsFgsFBcX8/r1a+LxOGVlZfr46tWrp6xVFAVVVbl06RJer5e1a9dO+1ldXV243W4URQHAbDZPO9/n85GdnQ3A06dP6e7u5uHDh8B4taKvr++Hcf7snnM6nTQ3NzMwMEAgEGDZsmVpnbecnBwcDgdLliwBwG63o6oqRUVF0x6rmHtsNhunT58mGo1y9epV1q1bx8aNG2lpacFsNrNq1SpycnIyHaYQMyYJ2QxNJE2qqg0YOBYAAAOeSURBVHLz5k1OnjyJ2WymoaGBb9++6fP++GP81BoMBsbGxvTx790i0ab5d6ILFy6ctHbidVZWFqlU6tcO5i+LFi0Cxvt5cnNzv9t3Vl1dzatXr4hGoxw9epQzZ87MymeLmZlur0wwm82cPXuWJ0+ecPv2bdrb2wkGg7MWw8R+mYjnwIEDeDyeSXN+9LDHz+65TZs24XA4iEajnDhxgpqaGkpKStKK+5+/JYPBMGu/H/F7DQ4OYjab2bx5M0ajkXv37rFz504aGxtpbW3Vk32n08nFixdRVVW/ZfmjPzaEyKS52ZE+D4yOjmI0GjGZTAwNDek9K9Nxu920tbUB0NnZycjIiD4eiUT4+vUriUSCSCSC2+2etVhtNhuqqtLf3w/A/fv3KS4unjLPZDKhKAodHR3A+MX27du3APT391NYWMiePXtYvHgxAwMDGI1Gvnz5Mmtxir+53W46OjpIpVIMDw8Ti8VwOBwUFRXx6NEjUqkUQ0NDPH/+fMra4eFhUqkUZWVl7N27lzdv3gD86/fldDqJxWKoqgowo14bj8fDnTt39KpWb28viUQirThh5nvuw4cPLF26lO3bt+Pz+Xj37l1a5038f3R3dxMOhwmFQly7do3KykoMBgNer5fOzk7Wr18PQF5eHtXV1Zw7d45QKMT58+czHLkQ05MK2U+y2+3Y7XYOHz6Moii4XK4frtm9ezf19fXU1tbidruxWq3A+G3NiooKwuEwMN7Uv2LFCv0C+auys7MJBoPU1dXpTf3btm377txDhw7R1NREc3MzyWSS8vJy7HY7V65coa+vD4CSkhIKCgqwWq3cuHGDUCgkTf2zzO/309XVRSgUAmD//v1YLBYCgQDPnj3Tm9kLCwsxmUyT1g4ODnLhwgW9ArRv3z4AKioqaGpq0pv6J/zzwqVpGnl5eRw7diytOLdu3YqqqtTW1urvFQqF0opzwkz23PXr12lra2PBggVYLBZ27dqV1nnr6elJ63jE3OfxeKZUZAGqqqqoqqqaNFZaWkppaenvCk2IX5KlpXMPRAgxZ0w8Tfb582fC4TDHjx/HYrFkOqwp5kucQggxF0iFTIh55tSpU4yMjJBMJqmsrJyzSc58iVMIIeYCqZDNc3fv3uXWrVuTxlwuFwcPHsxQROL/QPaVEEL8XpKQCSGEEEJkmDxlKYQQQgiRYZKQCSGEEEJkmCRkQgghhBAZJgmZEEIIIUSGSUImhBBCCJFhfwJAbAnr8bSpJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d67f53be80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d7eee32e10>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlens.visualization import corrmat\n",
    "corrmat(b.corr(), inflate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08512942, 0.04266877, 0.04954358, 0.02149066, 0.30872143])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_forest</th>\n",
       "      <th>logistic_regression</th>\n",
       "      <th>svc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.575923</td>\n",
       "      <td>0.476276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.482</td>\n",
       "      <td>0.665925</td>\n",
       "      <td>0.670352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.484278</td>\n",
       "      <td>0.424862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.733526</td>\n",
       "      <td>0.771753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.710266</td>\n",
       "      <td>0.645285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   random_forest  logistic_regression       svc\n",
       "0          0.418             0.575923  0.476276\n",
       "1          0.482             0.665925  0.670352\n",
       "2          0.300             0.484278  0.424862\n",
       "3          0.438             0.733526  0.771753\n",
       "4          0.600             0.710266  0.645285"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.490066\n",
       "1    0.606092\n",
       "2    0.403047\n",
       "3    0.647760\n",
       "4    0.651850\n",
       "dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_forest          0.772000\n",
       "logistic_regression    0.877577\n",
       "extra_tree             0.687000\n",
       "gradient_boosting      0.947752\n",
       "svc                    0.942471\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(b, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9282017627688299"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_forest          0.772000\n",
       "logistic_regression    0.629701\n",
       "extra_tree             0.653000\n",
       "gradient_boosting      0.861726\n",
       "svc                    0.769408\n",
       "Name: 732, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.iloc[732, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([732], dtype=int64),)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(m == m.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1251,  358],\n",
       "       [ 159,  148]], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, (a >= 0.5) * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1596,   13],\n",
       "       [ 301,    6]], dtype=int64)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, (m >= 0.5) * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8361169102296451, 0.7301670146137788)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, (m >= 0.5) * 1), accuracy_score(y_test, (a >= 0.5) * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d7f005fc88>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d7f005f5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "Processing layer-1             done | 00:16:44\n",
      "Processing layer-2             done | 00:00:15\n",
      "Fit complete                        | 00:17:03\n",
      "\n",
      "Predicting 2 layers\n",
      "Processing layer-1             done | 00:01:25\n",
      "Processing layer-2             done | 00:00:02\n",
      "Predict complete                    | 00:01:50\n",
      "\n",
      "Super Learner ROC-AUC score: 0.488\n"
     ]
    }
   ],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Instantiate the ensemble with 10 folds\n",
    "sl = SuperLearner(\n",
    "    folds=10,\n",
    "    random_state=123,\n",
    "    verbose=2,\n",
    "    backend=\"multiprocessing\"\n",
    ")\n",
    "\n",
    "# Add the base learners and the meta learner\n",
    "sl.add(list(base_learners.values()), proba=True) \n",
    "sl.add_meta(meta_learner, proba=True)\n",
    "\n",
    "# Train the ensemble\n",
    "sl.fit(X_train_u, y_train_u)\n",
    "\n",
    "# Predict the test set\n",
    "p_sl = sl.predict_proba(X_test)\n",
    "\n",
    "print(\"\\nSuper Learner ROC-AUC score: %.3f\" % roc_auc_score(y_test, p_sl[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6351336031241206"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cl = xgb.XGBClassifier(n_jobs=4)\n",
    "xgb_cl.fit(X_train_d, y_train_d)\n",
    "roc_auc_score(y_test, xgb_cl.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DMatrix\n",
    "DMatrix = xgb.DMatrix(data=X_train_u, label=y_train_u)\n",
    "\n",
    "# Run cv\n",
    "params = {\"objective\": \"binary:logistic\", \"max_depth\": 3}\n",
    "cv_results = xgb.cv(dtrain=DMatrix, params=params, nfold=5, num_boost_round=100, stratified=True, metrics=\"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.640197</td>\n",
       "      <td>0.009882</td>\n",
       "      <td>0.644413</td>\n",
       "      <td>0.001967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.660559</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.667695</td>\n",
       "      <td>0.002831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.664494</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.674007</td>\n",
       "      <td>0.002251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669810</td>\n",
       "      <td>0.005190</td>\n",
       "      <td>0.682914</td>\n",
       "      <td>0.005102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.675974</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.690131</td>\n",
       "      <td>0.003859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.680721</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>0.695508</td>\n",
       "      <td>0.003315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.685933</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.701589</td>\n",
       "      <td>0.002509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.690604</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.708339</td>\n",
       "      <td>0.002703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.694528</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.712615</td>\n",
       "      <td>0.001880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.697302</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>0.718544</td>\n",
       "      <td>0.000630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.701952</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.724112</td>\n",
       "      <td>0.001231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.706551</td>\n",
       "      <td>0.007511</td>\n",
       "      <td>0.728609</td>\n",
       "      <td>0.001788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.708789</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.731912</td>\n",
       "      <td>0.001814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.711411</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.002083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.714502</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>0.738365</td>\n",
       "      <td>0.002442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.717670</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.742865</td>\n",
       "      <td>0.002299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.720814</td>\n",
       "      <td>0.008081</td>\n",
       "      <td>0.746201</td>\n",
       "      <td>0.001773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.723727</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.749247</td>\n",
       "      <td>0.002474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.724885</td>\n",
       "      <td>0.008409</td>\n",
       "      <td>0.751618</td>\n",
       "      <td>0.002618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.726478</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0.755300</td>\n",
       "      <td>0.002150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.727918</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>0.757181</td>\n",
       "      <td>0.002232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.729709</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.760203</td>\n",
       "      <td>0.002802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.731971</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.763107</td>\n",
       "      <td>0.002449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.733956</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.766095</td>\n",
       "      <td>0.003014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.735560</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>0.768987</td>\n",
       "      <td>0.003163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.736826</td>\n",
       "      <td>0.007056</td>\n",
       "      <td>0.770650</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.737968</td>\n",
       "      <td>0.006281</td>\n",
       "      <td>0.772287</td>\n",
       "      <td>0.002583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.739299</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.774277</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.740364</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.776107</td>\n",
       "      <td>0.002228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.743216</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>0.779419</td>\n",
       "      <td>0.002844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.790650</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.847027</td>\n",
       "      <td>0.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.792058</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.848243</td>\n",
       "      <td>0.003485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.793010</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.849509</td>\n",
       "      <td>0.003950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.793474</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>0.850728</td>\n",
       "      <td>0.004377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.794635</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.852245</td>\n",
       "      <td>0.004557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.795681</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.853618</td>\n",
       "      <td>0.004039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.796447</td>\n",
       "      <td>0.004133</td>\n",
       "      <td>0.854460</td>\n",
       "      <td>0.004013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.797049</td>\n",
       "      <td>0.004443</td>\n",
       "      <td>0.855324</td>\n",
       "      <td>0.004646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.797972</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>0.856041</td>\n",
       "      <td>0.004426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.798681</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.857041</td>\n",
       "      <td>0.005345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.799812</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.858111</td>\n",
       "      <td>0.004868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.800668</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.859216</td>\n",
       "      <td>0.005220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.801768</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.860440</td>\n",
       "      <td>0.005037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.802402</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>0.861558</td>\n",
       "      <td>0.005377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.803413</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.862742</td>\n",
       "      <td>0.005247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.804298</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>0.863555</td>\n",
       "      <td>0.005473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.805146</td>\n",
       "      <td>0.005138</td>\n",
       "      <td>0.865238</td>\n",
       "      <td>0.005860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.806385</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.866232</td>\n",
       "      <td>0.005579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.807020</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.867295</td>\n",
       "      <td>0.005622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.807305</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>0.868035</td>\n",
       "      <td>0.005308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.807691</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>0.868954</td>\n",
       "      <td>0.005064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.808692</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.870285</td>\n",
       "      <td>0.005192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.809619</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.872037</td>\n",
       "      <td>0.005049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.810612</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.873166</td>\n",
       "      <td>0.004994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.811586</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.873989</td>\n",
       "      <td>0.005177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.812345</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.874992</td>\n",
       "      <td>0.005230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.813097</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.875697</td>\n",
       "      <td>0.005290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.814109</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.876788</td>\n",
       "      <td>0.004775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.877655</td>\n",
       "      <td>0.004743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.815339</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.878815</td>\n",
       "      <td>0.004406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-auc-mean  test-auc-std  train-auc-mean  train-auc-std\n",
       "0        0.640197      0.009882        0.644413       0.001967\n",
       "1        0.660559      0.007037        0.667695       0.002831\n",
       "2        0.664494      0.006079        0.674007       0.002251\n",
       "3        0.669810      0.005190        0.682914       0.005102\n",
       "4        0.675974      0.004584        0.690131       0.003859\n",
       "5        0.680721      0.006498        0.695508       0.003315\n",
       "6        0.685933      0.007071        0.701589       0.002509\n",
       "7        0.690604      0.006501        0.708339       0.002703\n",
       "8        0.694528      0.007776        0.712615       0.001880\n",
       "9        0.697302      0.008066        0.718544       0.000630\n",
       "10       0.701952      0.007669        0.724112       0.001231\n",
       "11       0.706551      0.007511        0.728609       0.001788\n",
       "12       0.708789      0.007207        0.731912       0.001814\n",
       "13       0.711411      0.007244        0.735386       0.002083\n",
       "14       0.714502      0.007090        0.738365       0.002442\n",
       "15       0.717670      0.008161        0.742865       0.002299\n",
       "16       0.720814      0.008081        0.746201       0.001773\n",
       "17       0.723727      0.008333        0.749247       0.002474\n",
       "18       0.724885      0.008409        0.751618       0.002618\n",
       "19       0.726478      0.007455        0.755300       0.002150\n",
       "20       0.727918      0.007647        0.757181       0.002232\n",
       "21       0.729709      0.007312        0.760203       0.002802\n",
       "22       0.731971      0.007605        0.763107       0.002449\n",
       "23       0.733956      0.007480        0.766095       0.003014\n",
       "24       0.735560      0.007963        0.768987       0.003163\n",
       "25       0.736826      0.007056        0.770650       0.003050\n",
       "26       0.737968      0.006281        0.772287       0.002583\n",
       "27       0.739299      0.006395        0.774277       0.001527\n",
       "28       0.740364      0.006804        0.776107       0.002228\n",
       "29       0.743216      0.008295        0.779419       0.002844\n",
       "..            ...           ...             ...            ...\n",
       "70       0.790650      0.004640        0.847027       0.003535\n",
       "71       0.792058      0.003942        0.848243       0.003485\n",
       "72       0.793010      0.003957        0.849509       0.003950\n",
       "73       0.793474      0.003553        0.850728       0.004377\n",
       "74       0.794635      0.004412        0.852245       0.004557\n",
       "75       0.795681      0.004247        0.853618       0.004039\n",
       "76       0.796447      0.004133        0.854460       0.004013\n",
       "77       0.797049      0.004443        0.855324       0.004646\n",
       "78       0.797972      0.004420        0.856041       0.004426\n",
       "79       0.798681      0.004458        0.857041       0.005345\n",
       "80       0.799812      0.003892        0.858111       0.004868\n",
       "81       0.800668      0.004164        0.859216       0.005220\n",
       "82       0.801768      0.004355        0.860440       0.005037\n",
       "83       0.802402      0.004399        0.861558       0.005377\n",
       "84       0.803413      0.005076        0.862742       0.005247\n",
       "85       0.804298      0.005338        0.863555       0.005473\n",
       "86       0.805146      0.005138        0.865238       0.005860\n",
       "87       0.806385      0.004840        0.866232       0.005579\n",
       "88       0.807020      0.004883        0.867295       0.005622\n",
       "89       0.807305      0.004737        0.868035       0.005308\n",
       "90       0.807691      0.004230        0.868954       0.005064\n",
       "91       0.808692      0.004047        0.870285       0.005192\n",
       "92       0.809619      0.003633        0.872037       0.005049\n",
       "93       0.810612      0.003483        0.873166       0.004994\n",
       "94       0.811586      0.002709        0.873989       0.005177\n",
       "95       0.812345      0.002501        0.874992       0.005230\n",
       "96       0.813097      0.002551        0.875697       0.005290\n",
       "97       0.814109      0.002649        0.876788       0.004775\n",
       "98       0.814867      0.002992        0.877655       0.004743\n",
       "99       0.815339      0.003009        0.878815       0.004406\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Conclusion</h2><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
